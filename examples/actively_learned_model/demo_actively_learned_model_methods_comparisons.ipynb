{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actively learned model for an ensemble of heteroscedastic regression with offline query comparison to baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uq360.algorithms.actively_learned_model import ActivelyLearnedModel\n",
    "from uq360.algorithms.ensemble_heteroscedastic_regression import EnsembleHeteroscedasticRegression\n",
    "from uq360.metrics import picp, mpiw, compute_regression_metrics\n",
    "from uq360.metrics import UncertaintyCharacteristicsCurve as ucc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset (offline)\n",
    "\n",
    "Based on Pestourie, R., Mroueh, Y., Nguyen, T. V., Das, P., & Johnson, S. G. (2020). Active learning of deep surrogates for PDEs: application to metasurface design. npj Computational Materials, 6(1), 1-7. We use supervised learning to train a surrogate model that, given the parameters of an already defined geometry parameterization, will predict the complex transmission above the geometry. This type of surrogate model is very useful in the context of decomposition methods, where the computational domain of a large-scale simulation is broken down into smaller pieces with same geometry parameterization and boundary conditions. In the case of Maxwell's equation, where the smaller problems have periodic boundary conditions with a period smaller than the wavelength, then the far-field above the geometry can be fully summarized by a single complex number--the complex transmission, which is obtained by solving an expensive PDE.\n",
    "\n",
    "In order to alleviate that, we build a dataset, which consists of inputs to the surrogate model (10 inputs for the definition of the geometry, and 3 inputs for the one hot encoding of 3 frequencies) and their corresponding complex transmission (a single complex number). In this example, we train a surrogate model of the PDE solver to predict the real part of the complex transmission, using the least amount of data possible owing to an uncertainty quantification method available in uq360. This enables us to train the surrogate in an active learning fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a database with pores data\n",
    "dataset = \"data_realpart_complextransmission.csv\"\n",
    "data_pd = pd.read_csv(dataset)\n",
    "\n",
    "X_data = data_pd.iloc[:, :-1].values\n",
    "y_labels = np.squeeze(data_pd.iloc[:, -1:].values, axis=1)\n",
    "y_labels = y_labels.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Offline sample and querry, two mandatory arguments (and the data):\n",
    "    - Position where to start sampling\n",
    "    - Number of points to sample\n",
    "'''\n",
    "def sample_(start_index, n_points, X_data=X_data):\n",
    "    return X_data[start_index:start_index+n_points,:]\n",
    "\n",
    "def querry_(start_index, n_points, y_labels=y_labels):\n",
    "    return y_labels[start_index:start_index+n_points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: for online application, the sample_ function is sampling at random, and the querry_ performs the online data exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surrogate model\n",
    "\n",
    "The model is an ensemble of heteroskedastic regressions.\n",
    "Where each heteroskedastic regression is trained via optimizing a negative Gaussian loglikelihood where the mean $\\mu(p)$ and the variance $\\sigma(p)$ both depend on the input of the surrogate model $p$:\n",
    "\n",
    "$$-\\sum_p \\log p_\\Theta (y|p) \\propto \\sum_p \\left[ \\log \\sigma(p) + \\frac{(y(p)-\\mu(p))^2}{2\\sigma(p)^2} \\right]$$\n",
    "\n",
    "where $\\Theta$ are the parameters of the neural network.\n",
    "\n",
    "The mean and variance of the ensemble are the pooled mean $\\mu_* = \\frac{1}{J} \\sum_i \\mu_i(p)$ and variance $\\sigma^2_*(p) = \\frac{1}{J}\\sum_i(\\sigma_i^2(p)+(\\mu_i^2(p)-\\mu_*^2(p)))$, where $J$ is the number of models in the ensemble and $i$ is the index of the model in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# define config for Heteroscedastic regression\n",
    "config_HR = {\"num_features\": 13, \"num_hidden\": 64, \"num_outputs\": 1, \"batch_size\": 16, \"num_epochs\": 10,\n",
    "                  \"lr\": 0.001}\n",
    "HR_kwargs = {\"model_type\":'mlp',\n",
    "               \"config\": config_HR,\n",
    "               \"device\": device}\n",
    "# define config for ensemble\n",
    "config_ensemble = {\"num_models\": 1, \n",
    "          \"batch_size\": 16,\n",
    "          \"model_kwargs\":HR_kwargs, }\n",
    "\n",
    "ninit = 128\n",
    "T = 8\n",
    "# define config for active learning object\n",
    "config_AL = {\"num_init\": 512, \n",
    " \"T\": 8, \n",
    " \"K\": 64, \n",
    " \"M\": 4, \n",
    " \"sampling_function\": sample_, \n",
    " \"querry_function\" : querry_,\n",
    " \"model_function\": EnsembleHeteroscedasticRegression,\n",
    " \"model_kwargs\": {\"model_type\":'ensembleheteroscedasticregression', \n",
    "                                             \"config\":config_ensemble, \n",
    "                                             \"device\":device}, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the data set has the good dimension\n",
    "assert(X_data.shape[0] >= config_AL[\"num_init\"] + config_AL[\"T\"]*config_AL[\"M\"]*config_AL[\"K\"])\n",
    "assert(X_data.shape[1] == config_HR[\"num_features\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.1742808148264885\n",
      "Epoch: 1, loss = 1.142091229557991\n",
      "Epoch: 2, loss = 1.118767850100994\n",
      "Epoch: 3, loss = 1.0984802544116974\n",
      "Epoch: 4, loss = 1.0818207897245884\n",
      "Epoch: 5, loss = 1.0679391659796238\n",
      "Epoch: 6, loss = 1.0568950548768044\n",
      "Epoch: 7, loss = 1.0479964725673199\n",
      "Epoch: 8, loss = 1.0408768765628338\n",
      "Epoch: 9, loss = 1.034744057804346\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.1637066453695297\n",
      "Epoch: 1, loss = 1.132659401744604\n",
      "Epoch: 2, loss = 1.1075685657560825\n",
      "Epoch: 3, loss = 1.086596217006445\n",
      "Epoch: 4, loss = 1.068942453712225\n",
      "Epoch: 5, loss = 1.0536361299455166\n",
      "Epoch: 6, loss = 1.041418381035328\n",
      "Epoch: 7, loss = 1.0325183384120464\n",
      "Epoch: 8, loss = 1.0242810398340225\n",
      "Epoch: 9, loss = 1.0162724517285824\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.1736429333686829\n",
      "Epoch: 1, loss = 1.1317177042365074\n",
      "Epoch: 2, loss = 1.1047362796962261\n",
      "Epoch: 3, loss = 1.082258515059948\n",
      "Epoch: 4, loss = 1.065674889832735\n",
      "Epoch: 5, loss = 1.0534808672964573\n",
      "Epoch: 6, loss = 1.0449412874877453\n",
      "Epoch: 7, loss = 1.038776334375143\n",
      "Epoch: 8, loss = 1.0338588617742062\n",
      "Epoch: 9, loss = 1.0292346142232418\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.2116591334342957\n",
      "Epoch: 1, loss = 1.1729458719491959\n",
      "Epoch: 2, loss = 1.1437326073646545\n",
      "Epoch: 3, loss = 1.1184061169624329\n",
      "Epoch: 4, loss = 1.0962286219000816\n",
      "Epoch: 5, loss = 1.077100019901991\n",
      "Epoch: 6, loss = 1.0615759789943695\n",
      "Epoch: 7, loss = 1.0490782968699932\n",
      "Epoch: 8, loss = 1.0395844019949436\n",
      "Epoch: 9, loss = 1.0317558459937572\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.1806319877505302\n",
      "Epoch: 1, loss = 1.1380909122526646\n",
      "Epoch: 2, loss = 1.106933157891035\n",
      "Epoch: 3, loss = 1.081702545285225\n",
      "Epoch: 4, loss = 1.0626269914209843\n",
      "Epoch: 5, loss = 1.0484578311443329\n",
      "Epoch: 6, loss = 1.0377102755010128\n",
      "Epoch: 7, loss = 1.0288546308875084\n",
      "Epoch: 8, loss = 1.0214033275842667\n",
      "Epoch: 9, loss = 1.0147158615291119\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.1296508461236952\n",
      "Epoch: 1, loss = 1.0948415572444599\n",
      "Epoch: 2, loss = 1.0739772965510688\n",
      "Epoch: 3, loss = 1.0616838013132412\n",
      "Epoch: 4, loss = 1.0531698788205783\n",
      "Epoch: 5, loss = 1.0472205330928168\n",
      "Epoch: 6, loss = 1.041977566977342\n",
      "Epoch: 7, loss = 1.0372546315193176\n",
      "Epoch: 8, loss = 1.0323144172628722\n",
      "Epoch: 9, loss = 1.0272404526670773\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.1302015781402588\n",
      "Epoch: 1, loss = 1.091026194393635\n",
      "Epoch: 2, loss = 1.0686569636066756\n",
      "Epoch: 3, loss = 1.0552395582199097\n",
      "Epoch: 4, loss = 1.046701247493426\n",
      "Epoch: 5, loss = 1.040141391257445\n",
      "Epoch: 6, loss = 1.0340753297011058\n",
      "Epoch: 7, loss = 1.028221326569716\n",
      "Epoch: 8, loss = 1.0224097967147827\n",
      "Epoch: 9, loss = 1.0162756567200026\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.1398617799083395\n",
      "Epoch: 1, loss = 1.1026069050033889\n",
      "Epoch: 2, loss = 1.0748805403709414\n",
      "Epoch: 3, loss = 1.0571167965730033\n",
      "Epoch: 4, loss = 1.0461667329072954\n",
      "Epoch: 5, loss = 1.0383500059445698\n",
      "Epoch: 6, loss = 1.032057762145996\n",
      "Epoch: 7, loss = 1.0261413902044298\n",
      "Epoch: 8, loss = 1.0202761863668757\n",
      "Epoch: 9, loss = 1.0144485409061115\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.1444262663523355\n",
      "Epoch: 1, loss = 1.100064545869827\n",
      "Epoch: 2, loss = 1.0715448533495266\n",
      "Epoch: 3, loss = 1.0549565528829892\n",
      "Epoch: 4, loss = 1.044745904703935\n",
      "Epoch: 5, loss = 1.0378198126951852\n",
      "Epoch: 6, loss = 1.0320517544945078\n",
      "Epoch: 7, loss = 1.026094342271487\n",
      "Epoch: 8, loss = 1.0198438962300618\n",
      "Epoch: 9, loss = 1.0136793926358223\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.1379098519682884\n",
      "Epoch: 1, loss = 1.0928392658631003\n",
      "Epoch: 2, loss = 1.0637353907028833\n",
      "Epoch: 3, loss = 1.046858511865139\n",
      "Epoch: 4, loss = 1.0364317099253337\n",
      "Epoch: 5, loss = 1.0286250114440918\n",
      "Epoch: 6, loss = 1.0215660581986108\n",
      "Epoch: 7, loss = 1.0146701311071713\n",
      "Epoch: 8, loss = 1.0078120479981103\n",
      "Epoch: 9, loss = 1.0006704628467562\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.0947465732693669\n",
      "Epoch: 1, loss = 1.0489547088742255\n",
      "Epoch: 2, loss = 1.0321261644363402\n",
      "Epoch: 3, loss = 1.0239069670438767\n",
      "Epoch: 4, loss = 1.01769792586565\n",
      "Epoch: 5, loss = 1.0113270714879035\n",
      "Epoch: 6, loss = 1.005102908611298\n",
      "Epoch: 7, loss = 0.9987100154161452\n",
      "Epoch: 8, loss = 0.9918266788125037\n",
      "Epoch: 9, loss = 0.9841327384114269\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.0929812952876092\n",
      "Epoch: 1, loss = 1.0407986626029018\n",
      "Epoch: 2, loss = 1.0239114195108412\n",
      "Epoch: 3, loss = 1.0149397075176239\n",
      "Epoch: 4, loss = 1.0071016728878022\n",
      "Epoch: 5, loss = 0.9996645525097847\n",
      "Epoch: 6, loss = 0.9926217809319497\n",
      "Epoch: 7, loss = 0.9850929319858552\n",
      "Epoch: 8, loss = 0.9762166589498521\n",
      "Epoch: 9, loss = 0.9669708386063575\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.1037530809640885\n",
      "Epoch: 1, loss = 1.0481454998254778\n",
      "Epoch: 2, loss = 1.024369740486145\n",
      "Epoch: 3, loss = 1.01421482861042\n",
      "Epoch: 4, loss = 1.0069991424679756\n",
      "Epoch: 5, loss = 0.9998861104249954\n",
      "Epoch: 6, loss = 0.9929743900895122\n",
      "Epoch: 7, loss = 0.9861349865794182\n",
      "Epoch: 8, loss = 0.9792334616184234\n",
      "Epoch: 9, loss = 0.9719429746270183\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.1102628946304325\n",
      "Epoch: 1, loss = 1.0478575199842453\n",
      "Epoch: 2, loss = 1.0251118153333663\n",
      "Epoch: 3, loss = 1.0154752030968666\n",
      "Epoch: 4, loss = 1.0084993332624435\n",
      "Epoch: 5, loss = 1.0019549250602722\n",
      "Epoch: 6, loss = 0.9950477644801141\n",
      "Epoch: 7, loss = 0.9867153003811836\n",
      "Epoch: 8, loss = 0.9754861891269685\n",
      "Epoch: 9, loss = 0.9625413566827774\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.0987871393561361\n",
      "Epoch: 1, loss = 1.0354186326265333\n",
      "Epoch: 2, loss = 1.0145006582140925\n",
      "Epoch: 3, loss = 1.0047051936388016\n",
      "Epoch: 4, loss = 0.9965243801474571\n",
      "Epoch: 5, loss = 0.988367035984993\n",
      "Epoch: 6, loss = 0.9797849699854851\n",
      "Epoch: 7, loss = 0.9707456603646277\n",
      "Epoch: 8, loss = 0.9612556248903275\n",
      "Epoch: 9, loss = 0.9509177818894387\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.0763205331232812\n",
      "Epoch: 1, loss = 1.038223491774665\n",
      "Epoch: 2, loss = 1.0253127589821813\n",
      "Epoch: 3, loss = 1.014852494001388\n",
      "Epoch: 4, loss = 1.0029880727330842\n",
      "Epoch: 5, loss = 0.9887991448243458\n",
      "Epoch: 6, loss = 0.9719661946098013\n",
      "Epoch: 7, loss = 0.9517329194479517\n",
      "Epoch: 8, loss = 0.9266756375630697\n",
      "Epoch: 9, loss = 0.8956264373328954\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.075350137220489\n",
      "Epoch: 1, loss = 1.0317050748401217\n",
      "Epoch: 2, loss = 1.0156544438666766\n",
      "Epoch: 3, loss = 1.0024316724803712\n",
      "Epoch: 4, loss = 0.9886591128177117\n",
      "Epoch: 5, loss = 0.9741140421893859\n",
      "Epoch: 6, loss = 0.9580344847506949\n",
      "Epoch: 7, loss = 0.9401047842370139\n",
      "Epoch: 8, loss = 0.9193391435676147\n",
      "Epoch: 9, loss = 0.8950905071364511\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.0834556619326274\n",
      "Epoch: 1, loss = 1.0320055335760117\n",
      "Epoch: 2, loss = 1.0148633751604292\n",
      "Epoch: 3, loss = 1.001714584728082\n",
      "Epoch: 4, loss = 0.988322784503301\n",
      "Epoch: 5, loss = 0.9739828432599705\n",
      "Epoch: 6, loss = 0.9580964363283584\n",
      "Epoch: 7, loss = 0.939937398665481\n",
      "Epoch: 8, loss = 0.9186467544900048\n",
      "Epoch: 9, loss = 0.8928650981850094\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.0838607938753229\n",
      "Epoch: 1, loss = 1.0294913583331637\n",
      "Epoch: 2, loss = 1.0114633937676747\n",
      "Epoch: 3, loss = 0.9930342502064174\n",
      "Epoch: 4, loss = 0.9742090760005843\n",
      "Epoch: 5, loss = 0.9533030067880949\n",
      "Epoch: 6, loss = 0.9285435097085103\n",
      "Epoch: 7, loss = 0.8979083581103217\n",
      "Epoch: 8, loss = 0.8628722462389206\n",
      "Epoch: 9, loss = 0.8230618163943291\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.0746831198533378\n",
      "Epoch: 1, loss = 1.0200196736388734\n",
      "Epoch: 2, loss = 1.0033834866351552\n",
      "Epoch: 3, loss = 0.9887155244747801\n",
      "Epoch: 4, loss = 0.9721268572741089\n",
      "Epoch: 5, loss = 0.9527166667911745\n",
      "Epoch: 6, loss = 0.9289319581455655\n",
      "Epoch: 7, loss = 0.9009106862876151\n",
      "Epoch: 8, loss = 0.868339148660501\n",
      "Epoch: 9, loss = 0.8307013172242375\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.0540174394845963\n",
      "Epoch: 1, loss = 1.0172565390958508\n",
      "Epoch: 2, loss = 0.9977110352586293\n",
      "Epoch: 3, loss = 0.9724470482153053\n",
      "Epoch: 4, loss = 0.937313491368995\n",
      "Epoch: 5, loss = 0.8868119190720954\n",
      "Epoch: 6, loss = 0.8138590279747455\n",
      "Epoch: 7, loss = 0.7249001847470508\n",
      "Epoch: 8, loss = 0.6559054899741623\n",
      "Epoch: 9, loss = 0.6253148670143942\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.0494716596954015\n",
      "Epoch: 1, loss = 1.0060999489882412\n",
      "Epoch: 2, loss = 0.9837099966757435\n",
      "Epoch: 3, loss = 0.9583964715985687\n",
      "Epoch: 4, loss = 0.9269357919692993\n",
      "Epoch: 5, loss = 0.8860416232663042\n",
      "Epoch: 6, loss = 0.8297399602392138\n",
      "Epoch: 7, loss = 0.7558325901627541\n",
      "Epoch: 8, loss = 0.6704075290438009\n",
      "Epoch: 9, loss = 0.6072926496100772\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.0541263902012032\n",
      "Epoch: 1, loss = 1.0060174697462254\n",
      "Epoch: 2, loss = 0.9831579003263925\n",
      "Epoch: 3, loss = 0.9580015974009739\n",
      "Epoch: 4, loss = 0.9264059093068627\n",
      "Epoch: 5, loss = 0.8844590660403756\n",
      "Epoch: 6, loss = 0.82780920670313\n",
      "Epoch: 7, loss = 0.7530894343029053\n",
      "Epoch: 8, loss = 0.6731480154044489\n",
      "Epoch: 9, loss = 0.6196456856149083\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.0547952410929344\n",
      "Epoch: 1, loss = 1.0055170813027552\n",
      "Epoch: 2, loss = 0.974114950527163\n",
      "Epoch: 3, loss = 0.9353573120692197\n",
      "Epoch: 4, loss = 0.882154679911978\n",
      "Epoch: 5, loss = 0.8138710242860453\n",
      "Epoch: 6, loss = 0.7292030828402319\n",
      "Epoch: 7, loss = 0.6466893937219592\n",
      "Epoch: 8, loss = 0.5971596689127825\n",
      "Epoch: 9, loss = 0.5698706194320144\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.044235610786606\n",
      "Epoch: 1, loss = 0.993528446292176\n",
      "Epoch: 2, loss = 0.9632464919020151\n",
      "Epoch: 3, loss = 0.9249492953805364\n",
      "Epoch: 4, loss = 0.8744300709051248\n",
      "Epoch: 5, loss = 0.8085991591215136\n",
      "Epoch: 6, loss = 0.7255663674543886\n",
      "Epoch: 7, loss = 0.6403477945748498\n",
      "Epoch: 8, loss = 0.586798903258408\n",
      "Epoch: 9, loss = 0.5592253296909963\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.0227588080998624\n",
      "Epoch: 1, loss = 0.9761995731881168\n",
      "Epoch: 2, loss = 0.9182731390676713\n",
      "Epoch: 3, loss = 0.8086220684715292\n",
      "Epoch: 4, loss = 0.6835242399776523\n",
      "Epoch: 5, loss = 0.625282175161622\n",
      "Epoch: 6, loss = 0.5913412160607\n",
      "Epoch: 7, loss = 0.5598787039405471\n",
      "Epoch: 8, loss = 0.5292880240771356\n",
      "Epoch: 9, loss = 0.5018656329872706\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.015311045628606\n",
      "Epoch: 1, loss = 0.9623448498772859\n",
      "Epoch: 2, loss = 0.8996595715483027\n",
      "Epoch: 3, loss = 0.7921855166328676\n",
      "Epoch: 4, loss = 0.6550990006971086\n",
      "Epoch: 5, loss = 0.574515361451741\n",
      "Epoch: 6, loss = 0.5267231653472694\n",
      "Epoch: 7, loss = 0.4813267996774592\n",
      "Epoch: 8, loss = 0.43538041926468857\n",
      "Epoch: 9, loss = 0.39836824471554305\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.0173896187634175\n",
      "Epoch: 1, loss = 0.9607968730005355\n",
      "Epoch: 2, loss = 0.9066428986914236\n",
      "Epoch: 3, loss = 0.817249473861673\n",
      "Epoch: 4, loss = 0.6883387740588546\n",
      "Epoch: 5, loss = 0.6012020725756885\n",
      "Epoch: 6, loss = 0.5539471403498091\n",
      "Epoch: 7, loss = 0.5133337987248192\n",
      "Epoch: 8, loss = 0.4731328346286761\n",
      "Epoch: 9, loss = 0.4307601511845312\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.0168247613491428\n",
      "Epoch: 1, loss = 0.9459330299586967\n",
      "Epoch: 2, loss = 0.8571518320928919\n",
      "Epoch: 3, loss = 0.7178249780765986\n",
      "Epoch: 4, loss = 0.606134358059728\n",
      "Epoch: 5, loss = 0.5527958151404606\n",
      "Epoch: 6, loss = 0.5111457638047413\n",
      "Epoch: 7, loss = 0.4714736179660331\n",
      "Epoch: 8, loss = 0.43299089638149396\n",
      "Epoch: 9, loss = 0.3964760989324672\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.006577124424053\n",
      "Epoch: 1, loss = 0.940049098070824\n",
      "Epoch: 2, loss = 0.855140452357856\n",
      "Epoch: 3, loss = 0.7204048037528996\n",
      "Epoch: 4, loss = 0.5996556677150006\n",
      "Epoch: 5, loss = 0.5450969313581787\n",
      "Epoch: 6, loss = 0.5055500211645706\n",
      "Epoch: 7, loss = 0.46954219321240526\n",
      "Epoch: 8, loss = 0.43349835348569554\n",
      "Epoch: 9, loss = 0.38630744069109113\n"
     ]
    }
   ],
   "source": [
    "# Baseline without AL\n",
    "K_train_list = [16, 32, 64, 128, 256, 512]\n",
    "frac_err_baseline = []\n",
    "N_test = 512\n",
    "\n",
    "for i in range(len(K_train_list)):\n",
    "    \n",
    "    # Update dictiorary to have no active learning and the correct amount of points\n",
    "    config_AL[\"model_kwargs\"][\"config\"][\"num_models\"] = 5\n",
    "    config_AL[\"num_init\"] = ninit + K_train_list[i] * T\n",
    "    config_AL[\"T\"] = 0\n",
    "\n",
    "    # Instantiate the class object and train the model\n",
    "    uq_model = ActivelyLearnedModel(config=config_AL, device=device, online=False)\n",
    "    uq_model = uq_model.fit()\n",
    "\n",
    "    # Create a test dataset\n",
    "    X_test = sample_(int(config_AL[\"num_init\"] + config_AL[\"T\"]*config_AL[\"M\"]*config_AL[\"K\"]), int(N_test))\n",
    "    y_test = querry_(int(config_AL[\"num_init\"] + config_AL[\"T\"]*config_AL[\"M\"]*config_AL[\"K\"]), int(N_test))\n",
    "    y_test = np.reshape(y_test, (-1,))\n",
    "\n",
    "    res = uq_model.predict(X_test)\n",
    "    y_test_pred = np.squeeze(res.y_mean, axis=1)\n",
    "    \n",
    "    frac_err_baseline.append(np.sqrt(np.sum(np.square(y_test - y_test_pred)))/np.sqrt(np.sum(np.square(y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active learning\n",
    "\n",
    "The active learning algorithm is a potential solution to the exploration exploitation trade-off. Instead of exploring the input space at random. The active learning algorithm explores only the $K$ points $p$ with highest model variance $\\sigma_*^2(p)$ out of $k\\times M$ points sampled at random. NB: any other uncertainty quantification of uq360 could be used as a filter for exploration in the same way as $\\sigma_*^2(p)$. Since the uncertainty is only used as an ordering measure, any increasing function of an uncertainty would work (such as the difference of the ends of a confidence interval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.1963923871517181\n",
      "Epoch: 1, loss = 1.1781985014677048\n",
      "Epoch: 2, loss = 1.1640804931521416\n",
      "Epoch: 3, loss = 1.151992343366146\n",
      "Epoch: 4, loss = 1.1411870568990707\n",
      "Epoch: 5, loss = 1.1315441280603409\n",
      "Epoch: 6, loss = 1.1228797882795334\n",
      "Epoch: 7, loss = 1.1152742505073547\n",
      "Epoch: 8, loss = 1.1086474433541298\n",
      "Epoch: 9, loss = 1.1027466654777527\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.2020178884267807\n",
      "Epoch: 1, loss = 1.1823408156633377\n",
      "Epoch: 2, loss = 1.1670318990945816\n",
      "Epoch: 3, loss = 1.1535324826836586\n",
      "Epoch: 4, loss = 1.1409879475831985\n",
      "Epoch: 5, loss = 1.129554532468319\n",
      "Epoch: 6, loss = 1.1193057894706726\n",
      "Epoch: 7, loss = 1.1101637110114098\n",
      "Epoch: 8, loss = 1.1019703969359398\n",
      "Epoch: 9, loss = 1.0947442203760147\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.215959295630455\n",
      "Epoch: 1, loss = 1.1958709955215454\n",
      "Epoch: 2, loss = 1.182001680135727\n",
      "Epoch: 3, loss = 1.1694466471672058\n",
      "Epoch: 4, loss = 1.157359391450882\n",
      "Epoch: 5, loss = 1.1455898359417915\n",
      "Epoch: 6, loss = 1.1342896595597267\n",
      "Epoch: 7, loss = 1.1236381232738495\n",
      "Epoch: 8, loss = 1.1141643524169922\n",
      "Epoch: 9, loss = 1.105368360877037\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.2138453274965286\n",
      "Epoch: 1, loss = 1.193967878818512\n",
      "Epoch: 2, loss = 1.1777970492839813\n",
      "Epoch: 3, loss = 1.1627441048622131\n",
      "Epoch: 4, loss = 1.1492508798837662\n",
      "Epoch: 5, loss = 1.1372401714324951\n",
      "Epoch: 6, loss = 1.1261292845010757\n",
      "Epoch: 7, loss = 1.1158414781093597\n",
      "Epoch: 8, loss = 1.1066676154732704\n",
      "Epoch: 9, loss = 1.0986376404762268\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.21263986825943\n",
      "Epoch: 1, loss = 1.1916678696870804\n",
      "Epoch: 2, loss = 1.1749736815690994\n",
      "Epoch: 3, loss = 1.159878358244896\n",
      "Epoch: 4, loss = 1.1462118476629257\n",
      "Epoch: 5, loss = 1.1339227557182312\n",
      "Epoch: 6, loss = 1.1230667680501938\n",
      "Epoch: 7, loss = 1.1130428165197372\n",
      "Epoch: 8, loss = 1.1040225625038147\n",
      "Epoch: 9, loss = 1.0959854871034622\n",
      "\n",
      "T = 0\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.1311321920818753\n",
      "Epoch: 1, loss = 1.1194946699672277\n",
      "Epoch: 2, loss = 1.1162745157877605\n",
      "Epoch: 3, loss = 1.112673607137468\n",
      "Epoch: 4, loss = 1.1086049411031935\n",
      "Epoch: 5, loss = 1.1050759090317621\n",
      "Epoch: 6, loss = 1.1018367873297799\n",
      "Epoch: 7, loss = 1.0986465281910367\n",
      "Epoch: 8, loss = 1.095258269045088\n",
      "Epoch: 9, loss = 1.0919051435258655\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.1280007031228805\n",
      "Epoch: 1, loss = 1.1137048734558954\n",
      "Epoch: 2, loss = 1.1097573637962341\n",
      "Epoch: 3, loss = 1.106003913614485\n",
      "Epoch: 4, loss = 1.1017044583956401\n",
      "Epoch: 5, loss = 1.0974696146117318\n",
      "Epoch: 6, loss = 1.0934347377883062\n",
      "Epoch: 7, loss = 1.0896649691793652\n",
      "Epoch: 8, loss = 1.0859332084655762\n",
      "Epoch: 9, loss = 1.0820346143510606\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.134498251809014\n",
      "Epoch: 1, loss = 1.1210085418489244\n",
      "Epoch: 2, loss = 1.1160250968403287\n",
      "Epoch: 3, loss = 1.111523237493303\n",
      "Epoch: 4, loss = 1.1067623032463922\n",
      "Epoch: 5, loss = 1.102264126141866\n",
      "Epoch: 6, loss = 1.0981699095831976\n",
      "Epoch: 7, loss = 1.0943416621949937\n",
      "Epoch: 8, loss = 1.090437842739953\n",
      "Epoch: 9, loss = 1.0864914589458041\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.1233480241563587\n",
      "Epoch: 1, loss = 1.1121513512399464\n",
      "Epoch: 2, loss = 1.1078572471936543\n",
      "Epoch: 3, loss = 1.1038254963027105\n",
      "Epoch: 4, loss = 1.099745081530677\n",
      "Epoch: 5, loss = 1.0959936711523266\n",
      "Epoch: 6, loss = 1.0924165712462532\n",
      "Epoch: 7, loss = 1.0888788104057312\n",
      "Epoch: 8, loss = 1.0852848158942328\n",
      "Epoch: 9, loss = 1.0815919637680054\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.126301798555586\n",
      "Epoch: 1, loss = 1.11497782336341\n",
      "Epoch: 2, loss = 1.1101691789097257\n",
      "Epoch: 3, loss = 1.10553867287106\n",
      "Epoch: 4, loss = 1.1008667018678453\n",
      "Epoch: 5, loss = 1.0964120361540053\n",
      "Epoch: 6, loss = 1.0922714802953932\n",
      "Epoch: 7, loss = 1.0882128940688238\n",
      "Epoch: 8, loss = 1.084025694264306\n",
      "Epoch: 9, loss = 1.079754518138038\n",
      "\n",
      "T = 1\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.0845865547657012\n",
      "Epoch: 1, loss = 1.0737044274806975\n",
      "Epoch: 2, loss = 1.0715937197208403\n",
      "Epoch: 3, loss = 1.0683597087860108\n",
      "Epoch: 4, loss = 1.065033596754074\n",
      "Epoch: 5, loss = 1.062104868888855\n",
      "Epoch: 6, loss = 1.0592974901199341\n",
      "Epoch: 7, loss = 1.05632826089859\n",
      "Epoch: 8, loss = 1.0532868206501007\n",
      "Epoch: 9, loss = 1.0502781689167022\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.0777454793453216\n",
      "Epoch: 1, loss = 1.0631873667240144\n",
      "Epoch: 2, loss = 1.0606213510036469\n",
      "Epoch: 3, loss = 1.0567096531391145\n",
      "Epoch: 4, loss = 1.0526161968708037\n",
      "Epoch: 5, loss = 1.048962491750717\n",
      "Epoch: 6, loss = 1.0455276012420653\n",
      "Epoch: 7, loss = 1.0419787108898162\n",
      "Epoch: 8, loss = 1.038365662097931\n",
      "Epoch: 9, loss = 1.0346705913543701\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.0816086173057557\n",
      "Epoch: 1, loss = 1.0691915690898894\n",
      "Epoch: 2, loss = 1.0666988372802735\n",
      "Epoch: 3, loss = 1.0628511667251588\n",
      "Epoch: 4, loss = 1.0587785840034485\n",
      "Epoch: 5, loss = 1.0551548302173615\n",
      "Epoch: 6, loss = 1.0518331170082094\n",
      "Epoch: 7, loss = 1.0481862127780914\n",
      "Epoch: 8, loss = 1.0444052875041963\n",
      "Epoch: 9, loss = 1.0406748354434967\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.0751123189926146\n",
      "Epoch: 1, loss = 1.064126205444336\n",
      "Epoch: 2, loss = 1.060806143283844\n",
      "Epoch: 3, loss = 1.057041692733765\n",
      "Epoch: 4, loss = 1.0530225396156312\n",
      "Epoch: 5, loss = 1.0493304550647737\n",
      "Epoch: 6, loss = 1.0457010030746459\n",
      "Epoch: 7, loss = 1.0419321596622466\n",
      "Epoch: 8, loss = 1.0380291640758514\n",
      "Epoch: 9, loss = 1.0340066969394683\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.0755054354667664\n",
      "Epoch: 1, loss = 1.06414315700531\n",
      "Epoch: 2, loss = 1.0599480509757995\n",
      "Epoch: 3, loss = 1.0552410542964936\n",
      "Epoch: 4, loss = 1.0504171550273895\n",
      "Epoch: 5, loss = 1.0458842933177945\n",
      "Epoch: 6, loss = 1.0413703739643096\n",
      "Epoch: 7, loss = 1.036647915840149\n",
      "Epoch: 8, loss = 1.0319213330745696\n",
      "Epoch: 9, loss = 1.0272587478160857\n",
      "\n",
      "T = 2\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.0600682171908293\n",
      "Epoch: 1, loss = 1.050446949221871\n",
      "Epoch: 2, loss = 1.0474371314048767\n",
      "Epoch: 3, loss = 1.0442443164912134\n",
      "Epoch: 4, loss = 1.0410691174593838\n",
      "Epoch: 5, loss = 1.0380189202048562\n",
      "Epoch: 6, loss = 1.0348840410059148\n",
      "Epoch: 7, loss = 1.0317390561103823\n",
      "Epoch: 8, loss = 1.0284450487657026\n",
      "Epoch: 9, loss = 1.025094530799172\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.0458822412924331\n",
      "Epoch: 1, loss = 1.0334073088385842\n",
      "Epoch: 2, loss = 1.0300853794271294\n",
      "Epoch: 3, loss = 1.0266394886103543\n",
      "Epoch: 4, loss = 1.0231397368691184\n",
      "Epoch: 5, loss = 1.0197657292539424\n",
      "Epoch: 6, loss = 1.016145727851174\n",
      "Epoch: 7, loss = 1.0124159780415622\n",
      "Epoch: 8, loss = 1.008564602244984\n",
      "Epoch: 9, loss = 1.0045690428126943\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.0529250881888648\n",
      "Epoch: 1, loss = 1.04236886176196\n",
      "Epoch: 2, loss = 1.0391803871501575\n",
      "Epoch: 3, loss = 1.0356162949041887\n",
      "Epoch: 4, loss = 1.031875030560927\n",
      "Epoch: 5, loss = 1.0283217592672866\n",
      "Epoch: 6, loss = 1.024944104931571\n",
      "Epoch: 7, loss = 1.0213184573433614\n",
      "Epoch: 8, loss = 1.0177236091006887\n",
      "Epoch: 9, loss = 1.014067530632019\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.0449274453249846\n",
      "Epoch: 1, loss = 1.0344141667539424\n",
      "Epoch: 2, loss = 1.0301904624158686\n",
      "Epoch: 3, loss = 1.0262833779508416\n",
      "Epoch: 4, loss = 1.0222736705433237\n",
      "Epoch: 5, loss = 1.0182738737626509\n",
      "Epoch: 6, loss = 1.0143905661322854\n",
      "Epoch: 7, loss = 1.010247208855369\n",
      "Epoch: 8, loss = 1.006165547804399\n",
      "Epoch: 9, loss = 1.0018700740554116\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.0351204005154695\n",
      "Epoch: 1, loss = 1.023905797438188\n",
      "Epoch: 2, loss = 1.0192333297296003\n",
      "Epoch: 3, loss = 1.0144620971246199\n",
      "Epoch: 4, loss = 1.0094894712621516\n",
      "Epoch: 5, loss = 1.0043941898779438\n",
      "Epoch: 6, loss = 0.9994991638443687\n",
      "Epoch: 7, loss = 0.9944902766834604\n",
      "Epoch: 8, loss = 0.9895025437528437\n",
      "Epoch: 9, loss = 0.9844785332679749\n",
      "\n",
      "T = 3\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.029435783624649\n",
      "Epoch: 1, loss = 1.0187989125649133\n",
      "Epoch: 2, loss = 1.0153792798519135\n",
      "Epoch: 3, loss = 1.0116103589534757\n",
      "Epoch: 4, loss = 1.008123050133387\n",
      "Epoch: 5, loss = 1.0046884467204413\n",
      "Epoch: 6, loss = 1.0009830296039581\n",
      "Epoch: 7, loss = 0.9972049196561177\n",
      "Epoch: 8, loss = 0.9935872455437977\n",
      "Epoch: 9, loss = 0.9896674553553264\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.0132670849561691\n",
      "Epoch: 1, loss = 0.9991412510474524\n",
      "Epoch: 2, loss = 0.9954780141512553\n",
      "Epoch: 3, loss = 0.9914982716242472\n",
      "Epoch: 4, loss = 0.9878060519695283\n",
      "Epoch: 5, loss = 0.9838700691858926\n",
      "Epoch: 6, loss = 0.9797347287336984\n",
      "Epoch: 7, loss = 0.9756157100200653\n",
      "Epoch: 8, loss = 0.9715779374043148\n",
      "Epoch: 9, loss = 0.9674499332904817\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.0196220378081005\n",
      "Epoch: 1, loss = 1.0078246643145878\n",
      "Epoch: 2, loss = 1.0043203830718994\n",
      "Epoch: 3, loss = 1.000499208768209\n",
      "Epoch: 4, loss = 0.996816724538803\n",
      "Epoch: 5, loss = 0.9932869027058282\n",
      "Epoch: 6, loss = 0.989382137854894\n",
      "Epoch: 7, loss = 0.9857332507769266\n",
      "Epoch: 8, loss = 0.9817961255709331\n",
      "Epoch: 9, loss = 0.9777457912762959\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.0094255308310192\n",
      "Epoch: 1, loss = 0.9965519358714421\n",
      "Epoch: 2, loss = 0.9924967139959338\n",
      "Epoch: 3, loss = 0.9880804220835369\n",
      "Epoch: 4, loss = 0.9837986926237742\n",
      "Epoch: 5, loss = 0.979423001408577\n",
      "Epoch: 6, loss = 0.9750678489605585\n",
      "Epoch: 7, loss = 0.9705253044764203\n",
      "Epoch: 8, loss = 0.9660371094942093\n",
      "Epoch: 9, loss = 0.9614661286274592\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.9906830092271169\n",
      "Epoch: 1, loss = 0.9776920080184937\n",
      "Epoch: 2, loss = 0.9729665617148082\n",
      "Epoch: 3, loss = 0.9680364926656085\n",
      "Epoch: 4, loss = 0.9630972246328989\n",
      "Epoch: 5, loss = 0.9581925670305889\n",
      "Epoch: 6, loss = 0.9532217731078466\n",
      "Epoch: 7, loss = 0.9480670541524887\n",
      "Epoch: 8, loss = 0.9429414321978886\n",
      "Epoch: 9, loss = 0.937615359822909\n",
      "\n",
      "T = 4\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.9884532690048218\n",
      "Epoch: 1, loss = 0.9778167651249813\n",
      "Epoch: 2, loss = 0.9738705937678996\n",
      "Epoch: 3, loss = 0.9701687326798071\n",
      "Epoch: 4, loss = 0.9664823642143837\n",
      "Epoch: 5, loss = 0.962670752635369\n",
      "Epoch: 6, loss = 0.9585672616958617\n",
      "Epoch: 7, loss = 0.9545507064232461\n",
      "Epoch: 8, loss = 0.9503078139745272\n",
      "Epoch: 9, loss = 0.9459682106971741\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.9658047144229596\n",
      "Epoch: 1, loss = 0.9521930997188273\n",
      "Epoch: 2, loss = 0.9478835142575778\n",
      "Epoch: 3, loss = 0.9436783561339747\n",
      "Epoch: 4, loss = 0.9393333517588103\n",
      "Epoch: 5, loss = 0.9349509569314809\n",
      "Epoch: 6, loss = 0.9305493464836707\n",
      "Epoch: 7, loss = 0.9262024210049558\n",
      "Epoch: 8, loss = 0.9215893011826735\n",
      "Epoch: 9, loss = 0.9170420949275676\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.9777034337703998\n",
      "Epoch: 1, loss = 0.9659600853919982\n",
      "Epoch: 2, loss = 0.9618004927268395\n",
      "Epoch: 3, loss = 0.9575487375259399\n",
      "Epoch: 4, loss = 0.9530872656748847\n",
      "Epoch: 5, loss = 0.948596505018381\n",
      "Epoch: 6, loss = 0.9437569242257338\n",
      "Epoch: 7, loss = 0.9389817989789522\n",
      "Epoch: 8, loss = 0.9342004610941961\n",
      "Epoch: 9, loss = 0.9295835861792929\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.9605132845731883\n",
      "Epoch: 1, loss = 0.9475346711965708\n",
      "Epoch: 2, loss = 0.9428635743948129\n",
      "Epoch: 3, loss = 0.9384657511344323\n",
      "Epoch: 4, loss = 0.9338409579717195\n",
      "Epoch: 5, loss = 0.9292557698029739\n",
      "Epoch: 6, loss = 0.9245815185400155\n",
      "Epoch: 7, loss = 0.9198242471768306\n",
      "Epoch: 8, loss = 0.9150795386387752\n",
      "Epoch: 9, loss = 0.9102003436822157\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.9374590378541212\n",
      "Epoch: 1, loss = 0.9242975665972782\n",
      "Epoch: 2, loss = 0.9190278328382053\n",
      "Epoch: 3, loss = 0.9139904150596033\n",
      "Epoch: 4, loss = 0.9091442869259762\n",
      "Epoch: 5, loss = 0.9041637915831345\n",
      "Epoch: 6, loss = 0.8991975188255309\n",
      "Epoch: 7, loss = 0.8941198816666236\n",
      "Epoch: 8, loss = 0.8890219239088205\n",
      "Epoch: 9, loss = 0.8838497033485999\n",
      "\n",
      "T = 5\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.972487245287214\n",
      "Epoch: 1, loss = 0.9603476226329802\n",
      "Epoch: 2, loss = 0.9551776221820285\n",
      "Epoch: 3, loss = 0.9506445697375707\n",
      "Epoch: 4, loss = 0.9465020171233586\n",
      "Epoch: 5, loss = 0.9421132973262242\n",
      "Epoch: 6, loss = 0.937613159418106\n",
      "Epoch: 7, loss = 0.9331143370696476\n",
      "Epoch: 8, loss = 0.9283694624900818\n",
      "Epoch: 9, loss = 0.9238730200699397\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.9465100467205048\n",
      "Epoch: 1, loss = 0.9290200514452799\n",
      "Epoch: 2, loss = 0.9252543789999824\n",
      "Epoch: 3, loss = 0.9203965791634151\n",
      "Epoch: 4, loss = 0.9153281279972623\n",
      "Epoch: 5, loss = 0.9099194407463073\n",
      "Epoch: 6, loss = 0.904908333505903\n",
      "Epoch: 7, loss = 0.8996625457491193\n",
      "Epoch: 8, loss = 0.8942986769335611\n",
      "Epoch: 9, loss = 0.8887333018439155\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.962052421910422\n",
      "Epoch: 1, loss = 0.9466445062841687\n",
      "Epoch: 2, loss = 0.9423264392784665\n",
      "Epoch: 3, loss = 0.9377282900469643\n",
      "Epoch: 4, loss = 0.9332460590771267\n",
      "Epoch: 5, loss = 0.9285517590386527\n",
      "Epoch: 6, loss = 0.9238480329513549\n",
      "Epoch: 7, loss = 0.9188878536224364\n",
      "Epoch: 8, loss = 0.9137394002505712\n",
      "Epoch: 9, loss = 0.9082786440849304\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.9423950697694506\n",
      "Epoch: 1, loss = 0.9252006326402936\n",
      "Epoch: 2, loss = 0.9212420838219778\n",
      "Epoch: 3, loss = 0.9164790979453495\n",
      "Epoch: 4, loss = 0.9115423560142517\n",
      "Epoch: 5, loss = 0.9064415608133588\n",
      "Epoch: 6, loss = 0.9014779755047388\n",
      "Epoch: 7, loss = 0.8962747922965458\n",
      "Epoch: 8, loss = 0.891090874161039\n",
      "Epoch: 9, loss = 0.8858690006392342\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.91374814084598\n",
      "Epoch: 1, loss = 0.8979419129235403\n",
      "Epoch: 2, loss = 0.8918361238070897\n",
      "Epoch: 3, loss = 0.8863198799746377\n",
      "Epoch: 4, loss = 0.8810062280723028\n",
      "Epoch: 5, loss = 0.8757212034293583\n",
      "Epoch: 6, loss = 0.8704310655593872\n",
      "Epoch: 7, loss = 0.8649430998734066\n",
      "Epoch: 8, loss = 0.8594794230801719\n",
      "Epoch: 9, loss = 0.8538660364491598\n",
      "\n",
      "T = 6\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.9373263200124106\n",
      "Epoch: 1, loss = 0.9250077168146769\n",
      "Epoch: 2, loss = 0.9202083269755046\n",
      "Epoch: 3, loss = 0.9153223156929018\n",
      "Epoch: 4, loss = 0.9110312620798746\n",
      "Epoch: 5, loss = 0.906109126408895\n",
      "Epoch: 6, loss = 0.9015607039133706\n",
      "Epoch: 7, loss = 0.8967318216959637\n",
      "Epoch: 8, loss = 0.8920520544052124\n",
      "Epoch: 9, loss = 0.8871617158253987\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.9057094971338907\n",
      "Epoch: 1, loss = 0.8883779048919678\n",
      "Epoch: 2, loss = 0.8837041656176251\n",
      "Epoch: 3, loss = 0.8773547609647115\n",
      "Epoch: 4, loss = 0.8711627364158631\n",
      "Epoch: 5, loss = 0.8646771272023519\n",
      "Epoch: 6, loss = 0.858253514766693\n",
      "Epoch: 7, loss = 0.8520270665486653\n",
      "Epoch: 8, loss = 0.8458481351534525\n",
      "Epoch: 9, loss = 0.8393492937088013\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.9221845348676044\n",
      "Epoch: 1, loss = 0.9084658821423849\n",
      "Epoch: 2, loss = 0.9030313650767008\n",
      "Epoch: 3, loss = 0.8979950507481893\n",
      "Epoch: 4, loss = 0.8928746461868285\n",
      "Epoch: 5, loss = 0.8877975106239318\n",
      "Epoch: 6, loss = 0.8825024406115213\n",
      "Epoch: 7, loss = 0.8773818532625833\n",
      "Epoch: 8, loss = 0.8720582922299702\n",
      "Epoch: 9, loss = 0.8668083826700845\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.9051507035891216\n",
      "Epoch: 1, loss = 0.8887638688087462\n",
      "Epoch: 2, loss = 0.8840140024820965\n",
      "Epoch: 3, loss = 0.8786684632301329\n",
      "Epoch: 4, loss = 0.8732594529787698\n",
      "Epoch: 5, loss = 0.8680063247680663\n",
      "Epoch: 6, loss = 0.862306523323059\n",
      "Epoch: 7, loss = 0.8568247695763906\n",
      "Epoch: 8, loss = 0.85130579272906\n",
      "Epoch: 9, loss = 0.8456310252348581\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.8715066432952882\n",
      "Epoch: 1, loss = 0.854662330945333\n",
      "Epoch: 2, loss = 0.849649143218994\n",
      "Epoch: 3, loss = 0.8439313411712647\n",
      "Epoch: 4, loss = 0.8386627515157065\n",
      "Epoch: 5, loss = 0.8330584605534872\n",
      "Epoch: 6, loss = 0.8277083098888398\n",
      "Epoch: 7, loss = 0.8222020626068114\n",
      "Epoch: 8, loss = 0.8167891999085743\n",
      "Epoch: 9, loss = 0.8111023704210916\n",
      "\n",
      "T = 7\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.8980069626122713\n",
      "Epoch: 1, loss = 0.8849567826837301\n",
      "Epoch: 2, loss = 0.8805100992321968\n",
      "Epoch: 3, loss = 0.8756579011678696\n",
      "Epoch: 4, loss = 0.8709818813949823\n",
      "Epoch: 5, loss = 0.8660871516913176\n",
      "Epoch: 6, loss = 0.8610584009438753\n",
      "Epoch: 7, loss = 0.8562275562435389\n",
      "Epoch: 8, loss = 0.8511860966682434\n",
      "Epoch: 9, loss = 0.8463300503790379\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.8496804442256689\n",
      "Epoch: 1, loss = 0.8305940292775631\n",
      "Epoch: 2, loss = 0.8260996639728546\n",
      "Epoch: 3, loss = 0.8188034147024155\n",
      "Epoch: 4, loss = 0.8129157293587923\n",
      "Epoch: 5, loss = 0.8061013258993626\n",
      "Epoch: 6, loss = 0.7999479975551367\n",
      "Epoch: 7, loss = 0.7932576332241297\n",
      "Epoch: 8, loss = 0.7867289260029793\n",
      "Epoch: 9, loss = 0.780422518029809\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.878414373844862\n",
      "Epoch: 1, loss = 0.8634873926639557\n",
      "Epoch: 2, loss = 0.8584367036819458\n",
      "Epoch: 3, loss = 0.8528723586350679\n",
      "Epoch: 4, loss = 0.8477870654314756\n",
      "Epoch: 5, loss = 0.8422414101660252\n",
      "Epoch: 6, loss = 0.8368568122386932\n",
      "Epoch: 7, loss = 0.8314453195780516\n",
      "Epoch: 8, loss = 0.8259953260421753\n",
      "Epoch: 9, loss = 0.8205893896520138\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.861082037910819\n",
      "Epoch: 1, loss = 0.8420752976089716\n",
      "Epoch: 2, loss = 0.8378899749368429\n",
      "Epoch: 3, loss = 0.8312028218060732\n",
      "Epoch: 4, loss = 0.8256098125129938\n",
      "Epoch: 5, loss = 0.8195028696209192\n",
      "Epoch: 6, loss = 0.8137568607926369\n",
      "Epoch: 7, loss = 0.8076547794044018\n",
      "Epoch: 8, loss = 0.8014122154563665\n",
      "Epoch: 9, loss = 0.7954419665038586\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.8249825611710548\n",
      "Epoch: 1, loss = 0.8074018489569426\n",
      "Epoch: 2, loss = 0.8018470909446478\n",
      "Epoch: 3, loss = 0.7960968911647797\n",
      "Epoch: 4, loss = 0.7900300603359938\n",
      "Epoch: 5, loss = 0.7844473514705896\n",
      "Epoch: 6, loss = 0.7784151528030634\n",
      "Epoch: 7, loss = 0.7728653755038977\n",
      "Epoch: 8, loss = 0.7664677444845438\n",
      "Epoch: 9, loss = 0.7611879631876945\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.1963923871517181\n",
      "Epoch: 1, loss = 1.1781985014677048\n",
      "Epoch: 2, loss = 1.1640804931521416\n",
      "Epoch: 3, loss = 1.151992343366146\n",
      "Epoch: 4, loss = 1.1411870568990707\n",
      "Epoch: 5, loss = 1.1315441280603409\n",
      "Epoch: 6, loss = 1.1228797882795334\n",
      "Epoch: 7, loss = 1.1152742505073547\n",
      "Epoch: 8, loss = 1.1086474433541298\n",
      "Epoch: 9, loss = 1.1027466654777527\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.2020178884267807\n",
      "Epoch: 1, loss = 1.1823408156633377\n",
      "Epoch: 2, loss = 1.1670318990945816\n",
      "Epoch: 3, loss = 1.1535324826836586\n",
      "Epoch: 4, loss = 1.1409879475831985\n",
      "Epoch: 5, loss = 1.129554532468319\n",
      "Epoch: 6, loss = 1.1193057894706726\n",
      "Epoch: 7, loss = 1.1101637110114098\n",
      "Epoch: 8, loss = 1.1019703969359398\n",
      "Epoch: 9, loss = 1.0947442203760147\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.215959295630455\n",
      "Epoch: 1, loss = 1.1958709955215454\n",
      "Epoch: 2, loss = 1.182001680135727\n",
      "Epoch: 3, loss = 1.1694466471672058\n",
      "Epoch: 4, loss = 1.157359391450882\n",
      "Epoch: 5, loss = 1.1455898359417915\n",
      "Epoch: 6, loss = 1.1342896595597267\n",
      "Epoch: 7, loss = 1.1236381232738495\n",
      "Epoch: 8, loss = 1.1141643524169922\n",
      "Epoch: 9, loss = 1.105368360877037\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.2138453274965286\n",
      "Epoch: 1, loss = 1.193967878818512\n",
      "Epoch: 2, loss = 1.1777970492839813\n",
      "Epoch: 3, loss = 1.1627441048622131\n",
      "Epoch: 4, loss = 1.1492508798837662\n",
      "Epoch: 5, loss = 1.1372401714324951\n",
      "Epoch: 6, loss = 1.1261292845010757\n",
      "Epoch: 7, loss = 1.1158414781093597\n",
      "Epoch: 8, loss = 1.1066676154732704\n",
      "Epoch: 9, loss = 1.0986376404762268\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.21263986825943\n",
      "Epoch: 1, loss = 1.1916678696870804\n",
      "Epoch: 2, loss = 1.1749736815690994\n",
      "Epoch: 3, loss = 1.159878358244896\n",
      "Epoch: 4, loss = 1.1462118476629257\n",
      "Epoch: 5, loss = 1.1339227557182312\n",
      "Epoch: 6, loss = 1.1230667680501938\n",
      "Epoch: 7, loss = 1.1130428165197372\n",
      "Epoch: 8, loss = 1.1040225625038147\n",
      "Epoch: 9, loss = 1.0959854871034622\n",
      "\n",
      "T = 0\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.1275959968566893\n",
      "Epoch: 1, loss = 1.1161541402339936\n",
      "Epoch: 2, loss = 1.1122648954391479\n",
      "Epoch: 3, loss = 1.108628451824188\n",
      "Epoch: 4, loss = 1.1050959706306458\n",
      "Epoch: 5, loss = 1.1018874168395996\n",
      "Epoch: 6, loss = 1.0987832903862\n",
      "Epoch: 7, loss = 1.0958229899406433\n",
      "Epoch: 8, loss = 1.0928603589534758\n",
      "Epoch: 9, loss = 1.0898182868957518\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.130823642015457\n",
      "Epoch: 1, loss = 1.1151858448982237\n",
      "Epoch: 2, loss = 1.1102468967437744\n",
      "Epoch: 3, loss = 1.1058486342430114\n",
      "Epoch: 4, loss = 1.1014405488967896\n",
      "Epoch: 5, loss = 1.0973708510398865\n",
      "Epoch: 6, loss = 1.0937541007995606\n",
      "Epoch: 7, loss = 1.0903154313564298\n",
      "Epoch: 8, loss = 1.086868667602539\n",
      "Epoch: 9, loss = 1.0834077358245848\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.1327722072601316\n",
      "Epoch: 1, loss = 1.118540745973587\n",
      "Epoch: 2, loss = 1.112899273633957\n",
      "Epoch: 3, loss = 1.108126163482666\n",
      "Epoch: 4, loss = 1.103454267978668\n",
      "Epoch: 5, loss = 1.099330097436905\n",
      "Epoch: 6, loss = 1.0957182168960569\n",
      "Epoch: 7, loss = 1.0923615038394927\n",
      "Epoch: 8, loss = 1.0889784693717957\n",
      "Epoch: 9, loss = 1.0855808496475219\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.1197348117828367\n",
      "Epoch: 1, loss = 1.1083668768405914\n",
      "Epoch: 2, loss = 1.1033953905105591\n",
      "Epoch: 3, loss = 1.0992127895355224\n",
      "Epoch: 4, loss = 1.095426195859909\n",
      "Epoch: 5, loss = 1.0919536292552947\n",
      "Epoch: 6, loss = 1.0886279046535492\n",
      "Epoch: 7, loss = 1.0854039251804353\n",
      "Epoch: 8, loss = 1.0823094964027407\n",
      "Epoch: 9, loss = 1.0791176438331602\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.130143803358078\n",
      "Epoch: 1, loss = 1.1180169224739074\n",
      "Epoch: 2, loss = 1.1122131288051604\n",
      "Epoch: 3, loss = 1.1073266744613646\n",
      "Epoch: 4, loss = 1.1028096020221712\n",
      "Epoch: 5, loss = 1.0986198008060455\n",
      "Epoch: 6, loss = 1.094603168964386\n",
      "Epoch: 7, loss = 1.0906342744827269\n",
      "Epoch: 8, loss = 1.0866319000720979\n",
      "Epoch: 9, loss = 1.0824714183807373\n",
      "\n",
      "T = 1\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.0896604508161545\n",
      "Epoch: 1, loss = 1.0807122538487115\n",
      "Epoch: 2, loss = 1.077951302131017\n",
      "Epoch: 3, loss = 1.0752310206492741\n",
      "Epoch: 4, loss = 1.0725742081801097\n",
      "Epoch: 5, loss = 1.0700783232847848\n",
      "Epoch: 6, loss = 1.0675930231809616\n",
      "Epoch: 7, loss = 1.064976399143537\n",
      "Epoch: 8, loss = 1.0623425195614498\n",
      "Epoch: 9, loss = 1.0596297184626262\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.0869258294502895\n",
      "Epoch: 1, loss = 1.0750586191813152\n",
      "Epoch: 2, loss = 1.0718792229890821\n",
      "Epoch: 3, loss = 1.068651720881462\n",
      "Epoch: 4, loss = 1.0655849973360698\n",
      "Epoch: 5, loss = 1.0627088497082393\n",
      "Epoch: 6, loss = 1.0599690030018487\n",
      "Epoch: 7, loss = 1.057056834300359\n",
      "Epoch: 8, loss = 1.0541715174913406\n",
      "Epoch: 9, loss = 1.0512285580237708\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.0877175877491634\n",
      "Epoch: 1, loss = 1.0772643784681957\n",
      "Epoch: 2, loss = 1.074547032515208\n",
      "Epoch: 3, loss = 1.0715463608503342\n",
      "Epoch: 4, loss = 1.0685254633426666\n",
      "Epoch: 5, loss = 1.0656742453575134\n",
      "Epoch: 6, loss = 1.0627685487270355\n",
      "Epoch: 7, loss = 1.0597975850105286\n",
      "Epoch: 8, loss = 1.0568340917428336\n",
      "Epoch: 9, loss = 1.0538152555624642\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.0829251855611801\n",
      "Epoch: 1, loss = 1.0732051978508632\n",
      "Epoch: 2, loss = 1.0694479495286942\n",
      "Epoch: 3, loss = 1.066256086031596\n",
      "Epoch: 4, loss = 1.0630489687124887\n",
      "Epoch: 5, loss = 1.059850424528122\n",
      "Epoch: 6, loss = 1.0567261377970378\n",
      "Epoch: 7, loss = 1.053500846028328\n",
      "Epoch: 8, loss = 1.0500548183918\n",
      "Epoch: 9, loss = 1.04668394724528\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.082490622997284\n",
      "Epoch: 1, loss = 1.072249213854472\n",
      "Epoch: 2, loss = 1.0679240574439368\n",
      "Epoch: 3, loss = 1.0639723787705102\n",
      "Epoch: 4, loss = 1.0600612610578537\n",
      "Epoch: 5, loss = 1.056234806776047\n",
      "Epoch: 6, loss = 1.0523187667131424\n",
      "Epoch: 7, loss = 1.0483153810103734\n",
      "Epoch: 8, loss = 1.0442100514968236\n",
      "Epoch: 9, loss = 1.039839267730713\n",
      "\n",
      "T = 2\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.0692467263766698\n",
      "Epoch: 1, loss = 1.0600319164139884\n",
      "Epoch: 2, loss = 1.0570490189961024\n",
      "Epoch: 3, loss = 1.054041236639023\n",
      "Epoch: 4, loss = 1.0513249252523693\n",
      "Epoch: 5, loss = 1.0485896893909996\n",
      "Epoch: 6, loss = 1.0457857634340013\n",
      "Epoch: 7, loss = 1.0429901948996954\n",
      "Epoch: 8, loss = 1.0399980672768183\n",
      "Epoch: 9, loss = 1.0371364269937788\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.0637115069798062\n",
      "Epoch: 1, loss = 1.0514924994536807\n",
      "Epoch: 2, loss = 1.0483393328530448\n",
      "Epoch: 3, loss = 1.0452794304915838\n",
      "Epoch: 4, loss = 1.0423801371029444\n",
      "Epoch: 5, loss = 1.0394373663834164\n",
      "Epoch: 6, loss = 1.0364153257438113\n",
      "Epoch: 7, loss = 1.0333561982427324\n",
      "Epoch: 8, loss = 1.0302831700869968\n",
      "Epoch: 9, loss = 1.0271822810173035\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.0685416374887737\n",
      "Epoch: 1, loss = 1.0572615223271506\n",
      "Epoch: 2, loss = 1.0544393658637998\n",
      "Epoch: 3, loss = 1.0515413710049222\n",
      "Epoch: 4, loss = 1.0487214795180728\n",
      "Epoch: 5, loss = 1.0459903819220406\n",
      "Epoch: 6, loss = 1.0431457587650843\n",
      "Epoch: 7, loss = 1.0401705248015267\n",
      "Epoch: 8, loss = 1.0372246887002672\n",
      "Epoch: 9, loss = 1.034115582704544\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.0590572442327226\n",
      "Epoch: 1, loss = 1.0480068751743863\n",
      "Epoch: 2, loss = 1.0443972817489078\n",
      "Epoch: 3, loss = 1.0411787245954787\n",
      "Epoch: 4, loss = 1.0378615983894892\n",
      "Epoch: 5, loss = 1.0345239468983243\n",
      "Epoch: 6, loss = 1.030993857554027\n",
      "Epoch: 7, loss = 1.0275157860347204\n",
      "Epoch: 8, loss = 1.0238421857357023\n",
      "Epoch: 9, loss = 1.0201263427734375\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.0518770728792464\n",
      "Epoch: 1, loss = 1.0399810203484128\n",
      "Epoch: 2, loss = 1.035330193383353\n",
      "Epoch: 3, loss = 1.0310247923646654\n",
      "Epoch: 4, loss = 1.0267987379005976\n",
      "Epoch: 5, loss = 1.0223893650940485\n",
      "Epoch: 6, loss = 1.0177399260657174\n",
      "Epoch: 7, loss = 1.013186284473964\n",
      "Epoch: 8, loss = 1.008431988103049\n",
      "Epoch: 9, loss = 1.0036294843469347\n",
      "\n",
      "T = 3\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.0388691686093807\n",
      "Epoch: 1, loss = 1.030660081654787\n",
      "Epoch: 2, loss = 1.0270663648843765\n",
      "Epoch: 3, loss = 1.0242550671100616\n",
      "Epoch: 4, loss = 1.0212987065315247\n",
      "Epoch: 5, loss = 1.0178699046373367\n",
      "Epoch: 6, loss = 1.0147224105894566\n",
      "Epoch: 7, loss = 1.0113088823854923\n",
      "Epoch: 8, loss = 1.0081378743052483\n",
      "Epoch: 9, loss = 1.0045124925673008\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.028581339865923\n",
      "Epoch: 1, loss = 1.0179662927985191\n",
      "Epoch: 2, loss = 1.0144217424094677\n",
      "Epoch: 3, loss = 1.0111930295825005\n",
      "Epoch: 4, loss = 1.007790219038725\n",
      "Epoch: 5, loss = 1.0042832046747208\n",
      "Epoch: 6, loss = 1.0004667155444622\n",
      "Epoch: 7, loss = 0.9965528883039951\n",
      "Epoch: 8, loss = 0.9926647134125233\n",
      "Epoch: 9, loss = 0.9887610226869583\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.035419523715973\n",
      "Epoch: 1, loss = 1.0260615907609463\n",
      "Epoch: 2, loss = 1.022582333534956\n",
      "Epoch: 3, loss = 1.0196419060230255\n",
      "Epoch: 4, loss = 1.0166550129652023\n",
      "Epoch: 5, loss = 1.013495061546564\n",
      "Epoch: 6, loss = 1.0102405026555061\n",
      "Epoch: 7, loss = 1.0069491490721703\n",
      "Epoch: 8, loss = 1.003798808902502\n",
      "Epoch: 9, loss = 1.000383898615837\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.0250799506902695\n",
      "Epoch: 1, loss = 1.014692299067974\n",
      "Epoch: 2, loss = 1.010408215224743\n",
      "Epoch: 3, loss = 1.0065190233290195\n",
      "Epoch: 4, loss = 1.0025805719196796\n",
      "Epoch: 5, loss = 0.9985720217227936\n",
      "Epoch: 6, loss = 0.9943342991173267\n",
      "Epoch: 7, loss = 0.9901198968291283\n",
      "Epoch: 8, loss = 0.985888309776783\n",
      "Epoch: 9, loss = 0.9816552475094795\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.0041941180825233\n",
      "Epoch: 1, loss = 0.992417935281992\n",
      "Epoch: 2, loss = 0.9873158484697342\n",
      "Epoch: 3, loss = 0.9824952185153961\n",
      "Epoch: 4, loss = 0.9776491858065128\n",
      "Epoch: 5, loss = 0.9727338217198849\n",
      "Epoch: 6, loss = 0.9675451368093491\n",
      "Epoch: 7, loss = 0.9623439200222492\n",
      "Epoch: 8, loss = 0.9571961238980293\n",
      "Epoch: 9, loss = 0.9520425833761692\n",
      "\n",
      "T = 4\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.012451234791014\n",
      "Epoch: 1, loss = 1.0030782719453175\n",
      "Epoch: 2, loss = 0.9993302590317196\n",
      "Epoch: 3, loss = 0.9959153930346172\n",
      "Epoch: 4, loss = 0.9927224914232888\n",
      "Epoch: 5, loss = 0.9891242451137967\n",
      "Epoch: 6, loss = 0.9856313400798373\n",
      "Epoch: 7, loss = 0.9818339149157206\n",
      "Epoch: 8, loss = 0.9781386587354871\n",
      "Epoch: 9, loss = 0.9742672542730967\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.9985429611470964\n",
      "Epoch: 1, loss = 0.9848268959257337\n",
      "Epoch: 2, loss = 0.9806280202335782\n",
      "Epoch: 3, loss = 0.9759806758827634\n",
      "Epoch: 4, loss = 0.9711256987518735\n",
      "Epoch: 5, loss = 0.9666231572628021\n",
      "Epoch: 6, loss = 0.9617264469464621\n",
      "Epoch: 7, loss = 0.9572313792175717\n",
      "Epoch: 8, loss = 0.9524320628907947\n",
      "Epoch: 9, loss = 0.9477862053447299\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.0101585851775277\n",
      "Epoch: 1, loss = 0.9991842640770806\n",
      "Epoch: 2, loss = 0.9955111344655357\n",
      "Epoch: 3, loss = 0.9922428296671972\n",
      "Epoch: 4, loss = 0.9887943632072873\n",
      "Epoch: 5, loss = 0.9850522147284614\n",
      "Epoch: 6, loss = 0.9814505477746328\n",
      "Epoch: 7, loss = 0.9776330954498714\n",
      "Epoch: 8, loss = 0.9738052586714425\n",
      "Epoch: 9, loss = 0.9697612623373667\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.9924446741739908\n",
      "Epoch: 1, loss = 0.9786166018909878\n",
      "Epoch: 2, loss = 0.9745503332879808\n",
      "Epoch: 3, loss = 0.970245361328125\n",
      "Epoch: 4, loss = 0.9654570718606312\n",
      "Epoch: 5, loss = 0.9608832034799786\n",
      "Epoch: 6, loss = 0.956083271238539\n",
      "Epoch: 7, loss = 0.9512277311748929\n",
      "Epoch: 8, loss = 0.9461834430694578\n",
      "Epoch: 9, loss = 0.9412212471167245\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.9654576083024343\n",
      "Epoch: 1, loss = 0.9509082370334201\n",
      "Epoch: 2, loss = 0.9464058710469139\n",
      "Epoch: 3, loss = 0.9416022863652971\n",
      "Epoch: 4, loss = 0.93658271100786\n",
      "Epoch: 5, loss = 0.9315859443611568\n",
      "Epoch: 6, loss = 0.9264473617076874\n",
      "Epoch: 7, loss = 0.9213886327213713\n",
      "Epoch: 8, loss = 0.91601535015636\n",
      "Epoch: 9, loss = 0.9109029240078395\n",
      "\n",
      "T = 5\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.0006933569908143\n",
      "Epoch: 1, loss = 0.9912118673324588\n",
      "Epoch: 2, loss = 0.9872142583131789\n",
      "Epoch: 3, loss = 0.983263638615608\n",
      "Epoch: 4, loss = 0.9790890544652939\n",
      "Epoch: 5, loss = 0.9747924059629439\n",
      "Epoch: 6, loss = 0.9700015366077424\n",
      "Epoch: 7, loss = 0.9657399594783784\n",
      "Epoch: 8, loss = 0.9607401579618451\n",
      "Epoch: 9, loss = 0.9566953241825105\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.9722103297710418\n",
      "Epoch: 1, loss = 0.9576239198446275\n",
      "Epoch: 2, loss = 0.9540062010288239\n",
      "Epoch: 3, loss = 0.9486237645149233\n",
      "Epoch: 4, loss = 0.9436702758073807\n",
      "Epoch: 5, loss = 0.9385840743780136\n",
      "Epoch: 6, loss = 0.9333818614482883\n",
      "Epoch: 7, loss = 0.9281069099903106\n",
      "Epoch: 8, loss = 0.9225836038589478\n",
      "Epoch: 9, loss = 0.9170639812946318\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.9973448306322099\n",
      "Epoch: 1, loss = 0.9855758786201477\n",
      "Epoch: 2, loss = 0.9813149452209473\n",
      "Epoch: 3, loss = 0.9762801915407182\n",
      "Epoch: 4, loss = 0.9717064529657364\n",
      "Epoch: 5, loss = 0.9670374989509581\n",
      "Epoch: 6, loss = 0.9626820713281633\n",
      "Epoch: 7, loss = 0.9581440657377245\n",
      "Epoch: 8, loss = 0.9534198999404908\n",
      "Epoch: 9, loss = 0.948519170284271\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.9696736752986908\n",
      "Epoch: 1, loss = 0.9551608830690385\n",
      "Epoch: 2, loss = 0.9501104354858396\n",
      "Epoch: 3, loss = 0.9444965362548828\n",
      "Epoch: 4, loss = 0.9388932168483736\n",
      "Epoch: 5, loss = 0.9331502974033355\n",
      "Epoch: 6, loss = 0.9275262147188187\n",
      "Epoch: 7, loss = 0.921673357486725\n",
      "Epoch: 8, loss = 0.9160567075014115\n",
      "Epoch: 9, loss = 0.9102864980697631\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.9426988363265993\n",
      "Epoch: 1, loss = 0.9283027648925781\n",
      "Epoch: 2, loss = 0.9235220640897751\n",
      "Epoch: 3, loss = 0.9180028051137925\n",
      "Epoch: 4, loss = 0.9130706280469894\n",
      "Epoch: 5, loss = 0.9072609692811965\n",
      "Epoch: 6, loss = 0.9021671205759049\n",
      "Epoch: 7, loss = 0.8962429940700531\n",
      "Epoch: 8, loss = 0.8911915600299835\n",
      "Epoch: 9, loss = 0.8849990010261535\n",
      "\n",
      "T = 6\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.970506711439653\n",
      "Epoch: 1, loss = 0.9601364921439779\n",
      "Epoch: 2, loss = 0.955224495042454\n",
      "Epoch: 3, loss = 0.9513472210277213\n",
      "Epoch: 4, loss = 0.9463340504602954\n",
      "Epoch: 5, loss = 0.942348604852503\n",
      "Epoch: 6, loss = 0.9372366233305498\n",
      "Epoch: 7, loss = 0.9333057539029556\n",
      "Epoch: 8, loss = 0.9278789352286947\n",
      "Epoch: 9, loss = 0.9242774058472027\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.9283704459667206\n",
      "Epoch: 1, loss = 0.9139422665942797\n",
      "Epoch: 2, loss = 0.9099633964625273\n",
      "Epoch: 3, loss = 0.9025675573132256\n",
      "Epoch: 4, loss = 0.8975340263410049\n",
      "Epoch: 5, loss = 0.8911973617293618\n",
      "Epoch: 6, loss = 0.8854108805006198\n",
      "Epoch: 7, loss = 0.8793550756844607\n",
      "Epoch: 8, loss = 0.8732443208044227\n",
      "Epoch: 9, loss = 0.8669255077838898\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.9638334783640775\n",
      "Epoch: 1, loss = 0.9516027353026648\n",
      "Epoch: 2, loss = 0.9473490471189672\n",
      "Epoch: 3, loss = 0.9418771158565173\n",
      "Epoch: 4, loss = 0.9370949187062004\n",
      "Epoch: 5, loss = 0.9318970441818236\n",
      "Epoch: 6, loss = 0.9269279729236256\n",
      "Epoch: 7, loss = 0.9214586940678683\n",
      "Epoch: 8, loss = 0.9162965498187325\n",
      "Epoch: 9, loss = 0.911019200628454\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.928185373544693\n",
      "Epoch: 1, loss = 0.9131121391599828\n",
      "Epoch: 2, loss = 0.9081491177732296\n",
      "Epoch: 3, loss = 0.9015623547814109\n",
      "Epoch: 4, loss = 0.8964091078801588\n",
      "Epoch: 5, loss = 0.8901264884255151\n",
      "Epoch: 6, loss = 0.8845164599743756\n",
      "Epoch: 7, loss = 0.8786798783323981\n",
      "Epoch: 8, loss = 0.8723237324844707\n",
      "Epoch: 9, loss = 0.8666750869967722\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.9041860374537383\n",
      "Epoch: 1, loss = 0.8892281488938764\n",
      "Epoch: 2, loss = 0.8847583071752027\n",
      "Epoch: 3, loss = 0.8781389160589738\n",
      "Epoch: 4, loss = 0.8734107261354271\n",
      "Epoch: 5, loss = 0.8665665117177097\n",
      "Epoch: 6, loss = 0.8616838211363012\n",
      "Epoch: 7, loss = 0.8548201742497357\n",
      "Epoch: 8, loss = 0.8496708829294551\n",
      "Epoch: 9, loss = 0.8429457613013009\n",
      "\n",
      "T = 7\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.9312397142251333\n",
      "Epoch: 1, loss = 0.9206366116801896\n",
      "Epoch: 2, loss = 0.9154182995359104\n",
      "Epoch: 3, loss = 0.9109326799710591\n",
      "Epoch: 4, loss = 0.9061276242136954\n",
      "Epoch: 5, loss = 0.9013279788196088\n",
      "Epoch: 6, loss = 0.896575540304184\n",
      "Epoch: 7, loss = 0.8915957448383173\n",
      "Epoch: 8, loss = 0.886632754156987\n",
      "Epoch: 9, loss = 0.8814929574728011\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.870571014781793\n",
      "Epoch: 1, loss = 0.855164426068465\n",
      "Epoch: 2, loss = 0.8492190713683763\n",
      "Epoch: 3, loss = 0.8401303241650262\n",
      "Epoch: 4, loss = 0.8358955755829811\n",
      "Epoch: 5, loss = 0.8263473436236382\n",
      "Epoch: 6, loss = 0.8212385723988217\n",
      "Epoch: 7, loss = 0.8129004277288913\n",
      "Epoch: 8, loss = 0.8068553569416205\n",
      "Epoch: 9, loss = 0.7985269042352836\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.9172677596410116\n",
      "Epoch: 1, loss = 0.9042739917834599\n",
      "Epoch: 2, loss = 0.8992076044281323\n",
      "Epoch: 3, loss = 0.8931505034367242\n",
      "Epoch: 4, loss = 0.8880669549107552\n",
      "Epoch: 5, loss = 0.8818820739785829\n",
      "Epoch: 6, loss = 0.8766082177559533\n",
      "Epoch: 7, loss = 0.8702327782909075\n",
      "Epoch: 8, loss = 0.8649994457761445\n",
      "Epoch: 9, loss = 0.8582384139299393\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.8739706066747507\n",
      "Epoch: 1, loss = 0.8582052712639173\n",
      "Epoch: 2, loss = 0.8523046200474103\n",
      "Epoch: 3, loss = 0.8438531855742137\n",
      "Epoch: 4, loss = 0.8398313435415425\n",
      "Epoch: 5, loss = 0.8304626271128653\n",
      "Epoch: 6, loss = 0.8258588251968225\n",
      "Epoch: 7, loss = 0.8170576045910516\n",
      "Epoch: 8, loss = 0.8119473693271478\n",
      "Epoch: 9, loss = 0.8037668777008853\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.852034439643224\n",
      "Epoch: 1, loss = 0.8367695820828278\n",
      "Epoch: 2, loss = 0.8303448259830476\n",
      "Epoch: 3, loss = 0.8234276982645192\n",
      "Epoch: 4, loss = 0.8171133510768414\n",
      "Epoch: 5, loss = 0.8103065490722657\n",
      "Epoch: 6, loss = 0.8038076733549435\n",
      "Epoch: 7, loss = 0.797169417142868\n",
      "Epoch: 8, loss = 0.7904105832179388\n",
      "Epoch: 9, loss = 0.7837196526428063\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.1963923871517181\n",
      "Epoch: 1, loss = 1.1781985014677048\n",
      "Epoch: 2, loss = 1.1640804931521416\n",
      "Epoch: 3, loss = 1.151992343366146\n",
      "Epoch: 4, loss = 1.1411870568990707\n",
      "Epoch: 5, loss = 1.1315441280603409\n",
      "Epoch: 6, loss = 1.1228797882795334\n",
      "Epoch: 7, loss = 1.1152742505073547\n",
      "Epoch: 8, loss = 1.1086474433541298\n",
      "Epoch: 9, loss = 1.1027466654777527\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.2020178884267807\n",
      "Epoch: 1, loss = 1.1823408156633377\n",
      "Epoch: 2, loss = 1.1670318990945816\n",
      "Epoch: 3, loss = 1.1535324826836586\n",
      "Epoch: 4, loss = 1.1409879475831985\n",
      "Epoch: 5, loss = 1.129554532468319\n",
      "Epoch: 6, loss = 1.1193057894706726\n",
      "Epoch: 7, loss = 1.1101637110114098\n",
      "Epoch: 8, loss = 1.1019703969359398\n",
      "Epoch: 9, loss = 1.0947442203760147\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.215959295630455\n",
      "Epoch: 1, loss = 1.1958709955215454\n",
      "Epoch: 2, loss = 1.182001680135727\n",
      "Epoch: 3, loss = 1.1694466471672058\n",
      "Epoch: 4, loss = 1.157359391450882\n",
      "Epoch: 5, loss = 1.1455898359417915\n",
      "Epoch: 6, loss = 1.1342896595597267\n",
      "Epoch: 7, loss = 1.1236381232738495\n",
      "Epoch: 8, loss = 1.1141643524169922\n",
      "Epoch: 9, loss = 1.105368360877037\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.2138453274965286\n",
      "Epoch: 1, loss = 1.193967878818512\n",
      "Epoch: 2, loss = 1.1777970492839813\n",
      "Epoch: 3, loss = 1.1627441048622131\n",
      "Epoch: 4, loss = 1.1492508798837662\n",
      "Epoch: 5, loss = 1.1372401714324951\n",
      "Epoch: 6, loss = 1.1261292845010757\n",
      "Epoch: 7, loss = 1.1158414781093597\n",
      "Epoch: 8, loss = 1.1066676154732704\n",
      "Epoch: 9, loss = 1.0986376404762268\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.21263986825943\n",
      "Epoch: 1, loss = 1.1916678696870804\n",
      "Epoch: 2, loss = 1.1749736815690994\n",
      "Epoch: 3, loss = 1.159878358244896\n",
      "Epoch: 4, loss = 1.1462118476629257\n",
      "Epoch: 5, loss = 1.1339227557182312\n",
      "Epoch: 6, loss = 1.1230667680501938\n",
      "Epoch: 7, loss = 1.1130428165197372\n",
      "Epoch: 8, loss = 1.1040225625038147\n",
      "Epoch: 9, loss = 1.0959854871034622\n",
      "\n",
      "T = 0\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.138370503981908\n",
      "Epoch: 1, loss = 1.1229194452365243\n",
      "Epoch: 2, loss = 1.1158660898605983\n",
      "Epoch: 3, loss = 1.1099179834127426\n",
      "Epoch: 4, loss = 1.1053806046644847\n",
      "Epoch: 5, loss = 1.1016465375820796\n",
      "Epoch: 6, loss = 1.0981623381376266\n",
      "Epoch: 7, loss = 1.09479824701945\n",
      "Epoch: 8, loss = 1.0916334042946496\n",
      "Epoch: 9, loss = 1.0884703348080316\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.1424744576215744\n",
      "Epoch: 1, loss = 1.1196245799462\n",
      "Epoch: 2, loss = 1.1110745469729104\n",
      "Epoch: 3, loss = 1.1040643602609634\n",
      "Epoch: 4, loss = 1.0986670901377997\n",
      "Epoch: 5, loss = 1.0945283075173693\n",
      "Epoch: 6, loss = 1.0910133918126423\n",
      "Epoch: 7, loss = 1.0874440222978592\n",
      "Epoch: 8, loss = 1.0836499383052192\n",
      "Epoch: 9, loss = 1.0797336250543597\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.1392671267191568\n",
      "Epoch: 1, loss = 1.1201953490575156\n",
      "Epoch: 2, loss = 1.112483302752177\n",
      "Epoch: 3, loss = 1.1052263279755912\n",
      "Epoch: 4, loss = 1.0994123717149098\n",
      "Epoch: 5, loss = 1.094896530111631\n",
      "Epoch: 6, loss = 1.0909540305534997\n",
      "Epoch: 7, loss = 1.0870435883601506\n",
      "Epoch: 8, loss = 1.0831899841626484\n",
      "Epoch: 9, loss = 1.07945849498113\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.1300002833207448\n",
      "Epoch: 1, loss = 1.113772953550021\n",
      "Epoch: 2, loss = 1.1056667119264603\n",
      "Epoch: 3, loss = 1.0995726734399796\n",
      "Epoch: 4, loss = 1.0948905448118846\n",
      "Epoch: 5, loss = 1.0909982919692993\n",
      "Epoch: 6, loss = 1.087665100892385\n",
      "Epoch: 7, loss = 1.0844762325286863\n",
      "Epoch: 8, loss = 1.0810156563917797\n",
      "Epoch: 9, loss = 1.077534516652425\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.1399957587321599\n",
      "Epoch: 1, loss = 1.1220208207766216\n",
      "Epoch: 2, loss = 1.1125252445538838\n",
      "Epoch: 3, loss = 1.1055204719305038\n",
      "Epoch: 4, loss = 1.0998505353927612\n",
      "Epoch: 5, loss = 1.0947359452644985\n",
      "Epoch: 6, loss = 1.090047245224317\n",
      "Epoch: 7, loss = 1.0854433725277581\n",
      "Epoch: 8, loss = 1.0807268569866815\n",
      "Epoch: 9, loss = 1.076056718826294\n",
      "\n",
      "T = 1\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.0950774364173412\n",
      "Epoch: 1, loss = 1.0845480263233185\n",
      "Epoch: 2, loss = 1.0799993202090263\n",
      "Epoch: 3, loss = 1.076520148664713\n",
      "Epoch: 4, loss = 1.0732259042561054\n",
      "Epoch: 5, loss = 1.07001244276762\n",
      "Epoch: 6, loss = 1.066993311047554\n",
      "Epoch: 7, loss = 1.0637965500354767\n",
      "Epoch: 8, loss = 1.060479510575533\n",
      "Epoch: 9, loss = 1.0571761280298233\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.0863086953759193\n",
      "Epoch: 1, loss = 1.0721092745661736\n",
      "Epoch: 2, loss = 1.0672072432935238\n",
      "Epoch: 3, loss = 1.0637665204703808\n",
      "Epoch: 4, loss = 1.0604755692183971\n",
      "Epoch: 5, loss = 1.0572648718953133\n",
      "Epoch: 6, loss = 1.0541620329022408\n",
      "Epoch: 7, loss = 1.0509113632142544\n",
      "Epoch: 8, loss = 1.047624945640564\n",
      "Epoch: 9, loss = 1.044325452297926\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.0888716131448746\n",
      "Epoch: 1, loss = 1.0768074989318848\n",
      "Epoch: 2, loss = 1.0721240378916264\n",
      "Epoch: 3, loss = 1.0685562901198864\n",
      "Epoch: 4, loss = 1.0653850845992565\n",
      "Epoch: 5, loss = 1.0621443465352058\n",
      "Epoch: 6, loss = 1.0589397698640823\n",
      "Epoch: 7, loss = 1.0557497031986713\n",
      "Epoch: 8, loss = 1.052514549344778\n",
      "Epoch: 9, loss = 1.049107987433672\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.0879477001726627\n",
      "Epoch: 1, loss = 1.0754659436643124\n",
      "Epoch: 2, loss = 1.070596233010292\n",
      "Epoch: 3, loss = 1.0667953416705132\n",
      "Epoch: 4, loss = 1.063322838395834\n",
      "Epoch: 5, loss = 1.0599435195326805\n",
      "Epoch: 6, loss = 1.0564270503818989\n",
      "Epoch: 7, loss = 1.0527461543679237\n",
      "Epoch: 8, loss = 1.049100261181593\n",
      "Epoch: 9, loss = 1.04532340914011\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.0835665836930275\n",
      "Epoch: 1, loss = 1.0706736780703068\n",
      "Epoch: 2, loss = 1.0651694498956203\n",
      "Epoch: 3, loss = 1.0603882037103176\n",
      "Epoch: 4, loss = 1.0558850429952145\n",
      "Epoch: 5, loss = 1.051526378840208\n",
      "Epoch: 6, loss = 1.0470810234546661\n",
      "Epoch: 7, loss = 1.0425817221403122\n",
      "Epoch: 8, loss = 1.037991065531969\n",
      "Epoch: 9, loss = 1.0333895459771156\n",
      "\n",
      "T = 2\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.0981095314025877\n",
      "Epoch: 1, loss = 1.0840011864900587\n",
      "Epoch: 2, loss = 1.0785242944955826\n",
      "Epoch: 3, loss = 1.0741060256958008\n",
      "Epoch: 4, loss = 1.07050094306469\n",
      "Epoch: 5, loss = 1.0665743619203567\n",
      "Epoch: 6, loss = 1.0629353374242785\n",
      "Epoch: 7, loss = 1.059453308582306\n",
      "Epoch: 8, loss = 1.0553423881530761\n",
      "Epoch: 9, loss = 1.051134866476059\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.0882004469633102\n",
      "Epoch: 1, loss = 1.0685587763786315\n",
      "Epoch: 2, loss = 1.0641291916370392\n",
      "Epoch: 3, loss = 1.0610460996627809\n",
      "Epoch: 4, loss = 1.0577501296997072\n",
      "Epoch: 5, loss = 1.0541654229164124\n",
      "Epoch: 6, loss = 1.0506340742111204\n",
      "Epoch: 7, loss = 1.046993198990822\n",
      "Epoch: 8, loss = 1.0434653192758558\n",
      "Epoch: 9, loss = 1.0396853506565094\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.0919080585241319\n",
      "Epoch: 1, loss = 1.0762923151254653\n",
      "Epoch: 2, loss = 1.0712062537670135\n",
      "Epoch: 3, loss = 1.0674068242311476\n",
      "Epoch: 4, loss = 1.0639563202857971\n",
      "Epoch: 5, loss = 1.060466730594635\n",
      "Epoch: 6, loss = 1.0568095445632935\n",
      "Epoch: 7, loss = 1.0531290233135224\n",
      "Epoch: 8, loss = 1.0493731439113616\n",
      "Epoch: 9, loss = 1.045489001274109\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.08644899725914\n",
      "Epoch: 1, loss = 1.0680824548006056\n",
      "Epoch: 2, loss = 1.0626860409975052\n",
      "Epoch: 3, loss = 1.0584666252136232\n",
      "Epoch: 4, loss = 1.0547303587198258\n",
      "Epoch: 5, loss = 1.0508338510990143\n",
      "Epoch: 6, loss = 1.0467190742492676\n",
      "Epoch: 7, loss = 1.0422164559364318\n",
      "Epoch: 8, loss = 1.0381684213876723\n",
      "Epoch: 9, loss = 1.0334520012140274\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.0745018571615221\n",
      "Epoch: 1, loss = 1.05607912838459\n",
      "Epoch: 2, loss = 1.051083579659462\n",
      "Epoch: 3, loss = 1.0468253552913667\n",
      "Epoch: 4, loss = 1.0425561279058455\n",
      "Epoch: 5, loss = 1.0380185395479204\n",
      "Epoch: 6, loss = 1.0335682958364487\n",
      "Epoch: 7, loss = 1.0287873953580857\n",
      "Epoch: 8, loss = 1.0237504959106445\n",
      "Epoch: 9, loss = 1.0183928996324543\n",
      "\n",
      "T = 3\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.066101392110189\n",
      "Epoch: 1, loss = 1.053967908024788\n",
      "Epoch: 2, loss = 1.0475317363937695\n",
      "Epoch: 3, loss = 1.0439650813738506\n",
      "Epoch: 4, loss = 1.0376727059483528\n",
      "Epoch: 5, loss = 1.0343568424383798\n",
      "Epoch: 6, loss = 1.0273623540997505\n",
      "Epoch: 7, loss = 1.023911419014136\n",
      "Epoch: 8, loss = 1.016247798999151\n",
      "Epoch: 9, loss = 1.013259234527747\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.0574967563152315\n",
      "Epoch: 1, loss = 1.039282701909542\n",
      "Epoch: 2, loss = 1.0356493790944417\n",
      "Epoch: 3, loss = 1.0306424150864284\n",
      "Epoch: 4, loss = 1.025127959748109\n",
      "Epoch: 5, loss = 1.0194774766763053\n",
      "Epoch: 6, loss = 1.0138405064741773\n",
      "Epoch: 7, loss = 1.0085201089580855\n",
      "Epoch: 8, loss = 1.002180109421412\n",
      "Epoch: 9, loss = 0.9969295238455135\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.0668623223900793\n",
      "Epoch: 1, loss = 1.0532798518737156\n",
      "Epoch: 2, loss = 1.048760799070199\n",
      "Epoch: 3, loss = 1.0442991430560746\n",
      "Epoch: 4, loss = 1.0398973549405737\n",
      "Epoch: 5, loss = 1.0349439159035683\n",
      "Epoch: 6, loss = 1.0302633047103882\n",
      "Epoch: 7, loss = 1.0249449188510575\n",
      "Epoch: 8, loss = 1.019599830110868\n",
      "Epoch: 9, loss = 1.0140939255555472\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.0524681384364762\n",
      "Epoch: 1, loss = 1.0363945464293163\n",
      "Epoch: 2, loss = 1.0306638727585475\n",
      "Epoch: 3, loss = 1.0262284204363823\n",
      "Epoch: 4, loss = 1.0207603797316553\n",
      "Epoch: 5, loss = 1.0155460064609847\n",
      "Epoch: 6, loss = 1.0100716774662335\n",
      "Epoch: 7, loss = 1.0041900699337325\n",
      "Epoch: 8, loss = 0.9981346651911737\n",
      "Epoch: 9, loss = 0.9922286147872605\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.0391293615102768\n",
      "Epoch: 1, loss = 1.0230401431520781\n",
      "Epoch: 2, loss = 1.0177467465400696\n",
      "Epoch: 3, loss = 1.0127081895867982\n",
      "Epoch: 4, loss = 1.0073493470748267\n",
      "Epoch: 5, loss = 1.0013440996408463\n",
      "Epoch: 6, loss = 0.9954744254549345\n",
      "Epoch: 7, loss = 0.9890197788675628\n",
      "Epoch: 8, loss = 0.9822636470198631\n",
      "Epoch: 9, loss = 0.9756325806180633\n",
      "\n",
      "T = 4\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.0239829782928738\n",
      "Epoch: 1, loss = 1.0156958933387485\n",
      "Epoch: 2, loss = 1.004803491490228\n",
      "Epoch: 3, loss = 1.0033396439892905\n",
      "Epoch: 4, loss = 0.9935411406414848\n",
      "Epoch: 5, loss = 0.9908883890935353\n",
      "Epoch: 6, loss = 0.9804539105721882\n",
      "Epoch: 7, loss = 0.9784810734646662\n",
      "Epoch: 8, loss = 0.9669484112943924\n",
      "Epoch: 9, loss = 0.9655678676707403\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.0091606868164882\n",
      "Epoch: 1, loss = 0.9921398631164005\n",
      "Epoch: 2, loss = 0.988034207906042\n",
      "Epoch: 3, loss = 0.9794973220143999\n",
      "Epoch: 4, loss = 0.9732607007026671\n",
      "Epoch: 5, loss = 0.9649299872773034\n",
      "Epoch: 6, loss = 0.9577928611210416\n",
      "Epoch: 7, loss = 0.9498840591737202\n",
      "Epoch: 8, loss = 0.9415794972862517\n",
      "Epoch: 9, loss = 0.9334763352360043\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.0276066405432565\n",
      "Epoch: 1, loss = 1.0144523956945963\n",
      "Epoch: 2, loss = 1.0094082717384611\n",
      "Epoch: 3, loss = 1.0027103019612178\n",
      "Epoch: 4, loss = 0.9966374146086827\n",
      "Epoch: 5, loss = 0.9893293082714081\n",
      "Epoch: 6, loss = 0.982910999229976\n",
      "Epoch: 7, loss = 0.9753088674374989\n",
      "Epoch: 8, loss = 0.9686791939394813\n",
      "Epoch: 9, loss = 0.9602769293955397\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.0081381499767303\n",
      "Epoch: 1, loss = 0.9939164455447878\n",
      "Epoch: 2, loss = 0.9869193945612226\n",
      "Epoch: 3, loss = 0.9802580795117787\n",
      "Epoch: 4, loss = 0.9735751641648154\n",
      "Epoch: 5, loss = 0.966507307120732\n",
      "Epoch: 6, loss = 0.9587172163384301\n",
      "Epoch: 7, loss = 0.9517713529723031\n",
      "Epoch: 8, loss = 0.9431493175881251\n",
      "Epoch: 9, loss = 0.93674364898886\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.9875435041529791\n",
      "Epoch: 1, loss = 0.974543273448944\n",
      "Epoch: 2, loss = 0.966345733829907\n",
      "Epoch: 3, loss = 0.9597175355468476\n",
      "Epoch: 4, loss = 0.95252397443567\n",
      "Epoch: 5, loss = 0.9446055889129641\n",
      "Epoch: 6, loss = 0.9372583031654358\n",
      "Epoch: 7, loss = 0.9289825452225551\n",
      "Epoch: 8, loss = 0.9209194417510712\n",
      "Epoch: 9, loss = 0.9125257496322904\n",
      "\n",
      "T = 5\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.9760192409157753\n",
      "Epoch: 1, loss = 0.9681398198008537\n",
      "Epoch: 2, loss = 0.9583008997142315\n",
      "Epoch: 3, loss = 0.9529847353696823\n",
      "Epoch: 4, loss = 0.9459795709699392\n",
      "Epoch: 5, loss = 0.9392019882798195\n",
      "Epoch: 6, loss = 0.9325482379645109\n",
      "Epoch: 7, loss = 0.9249795395880938\n",
      "Epoch: 8, loss = 0.91747329197824\n",
      "Epoch: 9, loss = 0.9100667014718056\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.9447439275681973\n",
      "Epoch: 1, loss = 0.9315585568547249\n",
      "Epoch: 2, loss = 0.9228914603590965\n",
      "Epoch: 3, loss = 0.9119727294892073\n",
      "Epoch: 4, loss = 0.9041588753461838\n",
      "Epoch: 5, loss = 0.893781254068017\n",
      "Epoch: 6, loss = 0.8843821175396442\n",
      "Epoch: 7, loss = 0.8748616315424442\n",
      "Epoch: 8, loss = 0.8640634045004845\n",
      "Epoch: 9, loss = 0.8549668826162815\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.9730821438133717\n",
      "Epoch: 1, loss = 0.9611776508390903\n",
      "Epoch: 2, loss = 0.9552125725895166\n",
      "Epoch: 3, loss = 0.9460316672921181\n",
      "Epoch: 4, loss = 0.9406233932822943\n",
      "Epoch: 5, loss = 0.9314454458653927\n",
      "Epoch: 6, loss = 0.9255946781486273\n",
      "Epoch: 7, loss = 0.916383245959878\n",
      "Epoch: 8, loss = 0.9096957631409168\n",
      "Epoch: 9, loss = 0.9009082708507776\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.9509082622826099\n",
      "Epoch: 1, loss = 0.9401594214141369\n",
      "Epoch: 2, loss = 0.9307300709187984\n",
      "Epoch: 3, loss = 0.9207759611308575\n",
      "Epoch: 4, loss = 0.9140835460275412\n",
      "Epoch: 5, loss = 0.9041129052639008\n",
      "Epoch: 6, loss = 0.895276365801692\n",
      "Epoch: 7, loss = 0.8863454107195139\n",
      "Epoch: 8, loss = 0.8767897561192513\n",
      "Epoch: 9, loss = 0.8669856190681458\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.9261490870267153\n",
      "Epoch: 1, loss = 0.9133434519171715\n",
      "Epoch: 2, loss = 0.9047375619411469\n",
      "Epoch: 3, loss = 0.8947804160416126\n",
      "Epoch: 4, loss = 0.8870810996741056\n",
      "Epoch: 5, loss = 0.8776326421648264\n",
      "Epoch: 6, loss = 0.8685734588652849\n",
      "Epoch: 7, loss = 0.8586438782513142\n",
      "Epoch: 8, loss = 0.8494989965111017\n",
      "Epoch: 9, loss = 0.8396917451173067\n",
      "\n",
      "T = 6\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.921391033464008\n",
      "Epoch: 1, loss = 0.9109968824519052\n",
      "Epoch: 2, loss = 0.902483668592241\n",
      "Epoch: 3, loss = 0.8958413998285931\n",
      "Epoch: 4, loss = 0.8883119490411548\n",
      "Epoch: 5, loss = 0.8807448032829497\n",
      "Epoch: 6, loss = 0.8734547189540333\n",
      "Epoch: 7, loss = 0.8652026901642482\n",
      "Epoch: 8, loss = 0.857798692252901\n",
      "Epoch: 9, loss = 0.8498521066374249\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.8570023096270032\n",
      "Epoch: 1, loss = 0.8474438985188802\n",
      "Epoch: 2, loss = 0.8267292380332946\n",
      "Epoch: 3, loss = 0.8214682721429399\n",
      "Epoch: 4, loss = 0.8072777109013664\n",
      "Epoch: 5, loss = 0.7974658856789272\n",
      "Epoch: 6, loss = 0.785798951983452\n",
      "Epoch: 7, loss = 0.7738988101482394\n",
      "Epoch: 8, loss = 0.7626334677139919\n",
      "Epoch: 9, loss = 0.7504330227772396\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.9112301137712268\n",
      "Epoch: 1, loss = 0.8993440022071202\n",
      "Epoch: 2, loss = 0.8893667343589995\n",
      "Epoch: 3, loss = 0.8819159468015033\n",
      "Epoch: 4, loss = 0.8732115791903599\n",
      "Epoch: 5, loss = 0.8646020086275203\n",
      "Epoch: 6, loss = 0.8566001330812771\n",
      "Epoch: 7, loss = 0.8474292134245236\n",
      "Epoch: 8, loss = 0.838161831928624\n",
      "Epoch: 9, loss = 0.8297181394365098\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.8763741610778705\n",
      "Epoch: 1, loss = 0.8662340334720079\n",
      "Epoch: 2, loss = 0.8499188464548851\n",
      "Epoch: 3, loss = 0.8418772071599958\n",
      "Epoch: 4, loss = 0.8305797154704728\n",
      "Epoch: 5, loss = 0.8200439049137962\n",
      "Epoch: 6, loss = 0.8094166053666009\n",
      "Epoch: 7, loss = 0.7977691913644472\n",
      "Epoch: 8, loss = 0.7869479291968876\n",
      "Epoch: 9, loss = 0.776033642391364\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.8518105919162435\n",
      "Epoch: 1, loss = 0.83683696637551\n",
      "Epoch: 2, loss = 0.8239452896846666\n",
      "Epoch: 3, loss = 0.8149287493692504\n",
      "Epoch: 4, loss = 0.804695539176464\n",
      "Epoch: 5, loss = 0.7938455707497066\n",
      "Epoch: 6, loss = 0.7837208981315293\n",
      "Epoch: 7, loss = 0.7726884567075306\n",
      "Epoch: 8, loss = 0.7625834229919647\n",
      "Epoch: 9, loss = 0.7515486876169841\n",
      "\n",
      "T = 7\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.8624153196811675\n",
      "Epoch: 1, loss = 0.847504061460495\n",
      "Epoch: 2, loss = 0.8389051444828509\n",
      "Epoch: 3, loss = 0.8296193070709708\n",
      "Epoch: 4, loss = 0.8210856720805167\n",
      "Epoch: 5, loss = 0.8117364801466468\n",
      "Epoch: 6, loss = 0.8033154807984828\n",
      "Epoch: 7, loss = 0.7934490978717805\n",
      "Epoch: 8, loss = 0.7843888498842715\n",
      "Epoch: 9, loss = 0.7740457698702811\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.7516738213598729\n",
      "Epoch: 1, loss = 0.7450909234583379\n",
      "Epoch: 2, loss = 0.7211169593036175\n",
      "Epoch: 3, loss = 0.7120255179703235\n",
      "Epoch: 4, loss = 0.6989848800003531\n",
      "Epoch: 5, loss = 0.6868878915905952\n",
      "Epoch: 6, loss = 0.6750429749488831\n",
      "Epoch: 7, loss = 0.6633478417992591\n",
      "Epoch: 8, loss = 0.6516517609357834\n",
      "Epoch: 9, loss = 0.6398934334516524\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.8394391991198062\n",
      "Epoch: 1, loss = 0.8275783076882361\n",
      "Epoch: 2, loss = 0.8141643606126305\n",
      "Epoch: 3, loss = 0.8059269636869432\n",
      "Epoch: 4, loss = 0.7953337676823139\n",
      "Epoch: 5, loss = 0.7858204536139964\n",
      "Epoch: 6, loss = 0.7756832338869574\n",
      "Epoch: 7, loss = 0.7654959462583065\n",
      "Epoch: 8, loss = 0.7553504765033722\n",
      "Epoch: 9, loss = 0.745158851146698\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.7840540885925293\n",
      "Epoch: 1, loss = 0.771367709338665\n",
      "Epoch: 2, loss = 0.7559288375079631\n",
      "Epoch: 3, loss = 0.7441497951745987\n",
      "Epoch: 4, loss = 0.7332701958715917\n",
      "Epoch: 5, loss = 0.7214635230600834\n",
      "Epoch: 6, loss = 0.7107838429510595\n",
      "Epoch: 7, loss = 0.6993839234113695\n",
      "Epoch: 8, loss = 0.6888528414070607\n",
      "Epoch: 9, loss = 0.6769359394907951\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.7623106636106967\n",
      "Epoch: 1, loss = 0.747474794089794\n",
      "Epoch: 2, loss = 0.7351654760539533\n",
      "Epoch: 3, loss = 0.723824366182089\n",
      "Epoch: 4, loss = 0.7127898290753363\n",
      "Epoch: 5, loss = 0.7013402435928584\n",
      "Epoch: 6, loss = 0.6902111411094665\n",
      "Epoch: 7, loss = 0.6788362592458724\n",
      "Epoch: 8, loss = 0.6674246616661549\n",
      "Epoch: 9, loss = 0.6561798457056285\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.1963923871517181\n",
      "Epoch: 1, loss = 1.1781985014677048\n",
      "Epoch: 2, loss = 1.1640804931521416\n",
      "Epoch: 3, loss = 1.151992343366146\n",
      "Epoch: 4, loss = 1.1411870568990707\n",
      "Epoch: 5, loss = 1.1315441280603409\n",
      "Epoch: 6, loss = 1.1228797882795334\n",
      "Epoch: 7, loss = 1.1152742505073547\n",
      "Epoch: 8, loss = 1.1086474433541298\n",
      "Epoch: 9, loss = 1.1027466654777527\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.2020178884267807\n",
      "Epoch: 1, loss = 1.1823408156633377\n",
      "Epoch: 2, loss = 1.1670318990945816\n",
      "Epoch: 3, loss = 1.1535324826836586\n",
      "Epoch: 4, loss = 1.1409879475831985\n",
      "Epoch: 5, loss = 1.129554532468319\n",
      "Epoch: 6, loss = 1.1193057894706726\n",
      "Epoch: 7, loss = 1.1101637110114098\n",
      "Epoch: 8, loss = 1.1019703969359398\n",
      "Epoch: 9, loss = 1.0947442203760147\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.215959295630455\n",
      "Epoch: 1, loss = 1.1958709955215454\n",
      "Epoch: 2, loss = 1.182001680135727\n",
      "Epoch: 3, loss = 1.1694466471672058\n",
      "Epoch: 4, loss = 1.157359391450882\n",
      "Epoch: 5, loss = 1.1455898359417915\n",
      "Epoch: 6, loss = 1.1342896595597267\n",
      "Epoch: 7, loss = 1.1236381232738495\n",
      "Epoch: 8, loss = 1.1141643524169922\n",
      "Epoch: 9, loss = 1.105368360877037\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.2138453274965286\n",
      "Epoch: 1, loss = 1.193967878818512\n",
      "Epoch: 2, loss = 1.1777970492839813\n",
      "Epoch: 3, loss = 1.1627441048622131\n",
      "Epoch: 4, loss = 1.1492508798837662\n",
      "Epoch: 5, loss = 1.1372401714324951\n",
      "Epoch: 6, loss = 1.1261292845010757\n",
      "Epoch: 7, loss = 1.1158414781093597\n",
      "Epoch: 8, loss = 1.1066676154732704\n",
      "Epoch: 9, loss = 1.0986376404762268\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.21263986825943\n",
      "Epoch: 1, loss = 1.1916678696870804\n",
      "Epoch: 2, loss = 1.1749736815690994\n",
      "Epoch: 3, loss = 1.159878358244896\n",
      "Epoch: 4, loss = 1.1462118476629257\n",
      "Epoch: 5, loss = 1.1339227557182312\n",
      "Epoch: 6, loss = 1.1230667680501938\n",
      "Epoch: 7, loss = 1.1130428165197372\n",
      "Epoch: 8, loss = 1.1040225625038147\n",
      "Epoch: 9, loss = 1.0959854871034622\n",
      "\n",
      "T = 0\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.1314434446394444\n",
      "Epoch: 1, loss = 1.1094308979809284\n",
      "Epoch: 2, loss = 1.0984203703701496\n",
      "Epoch: 3, loss = 1.091047927737236\n",
      "Epoch: 4, loss = 1.086034681648016\n",
      "Epoch: 5, loss = 1.0819141454994678\n",
      "Epoch: 6, loss = 1.0782111659646034\n",
      "Epoch: 7, loss = 1.0746699906885624\n",
      "Epoch: 8, loss = 1.07114976644516\n",
      "Epoch: 9, loss = 1.0674211010336876\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.1355453804135323\n",
      "Epoch: 1, loss = 1.103988904505968\n",
      "Epoch: 2, loss = 1.09079921990633\n",
      "Epoch: 3, loss = 1.0828284956514835\n",
      "Epoch: 4, loss = 1.0779308825731277\n",
      "Epoch: 5, loss = 1.0739437714219093\n",
      "Epoch: 6, loss = 1.0701159425079823\n",
      "Epoch: 7, loss = 1.0662114471197128\n",
      "Epoch: 8, loss = 1.0622714161872864\n",
      "Epoch: 9, loss = 1.0579658299684525\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.134122483432293\n",
      "Epoch: 1, loss = 1.108586423099041\n",
      "Epoch: 2, loss = 1.0968601070344448\n",
      "Epoch: 3, loss = 1.0880759358406067\n",
      "Epoch: 4, loss = 1.0819239430129528\n",
      "Epoch: 5, loss = 1.0770874731242657\n",
      "Epoch: 6, loss = 1.072830967605114\n",
      "Epoch: 7, loss = 1.0687297768890858\n",
      "Epoch: 8, loss = 1.0646569207310677\n",
      "Epoch: 9, loss = 1.060714639723301\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.1255055591464043\n",
      "Epoch: 1, loss = 1.101525079458952\n",
      "Epoch: 2, loss = 1.0892448760569096\n",
      "Epoch: 3, loss = 1.0819002985954285\n",
      "Epoch: 4, loss = 1.0775635465979576\n",
      "Epoch: 5, loss = 1.0740115903317928\n",
      "Epoch: 6, loss = 1.0706662237644196\n",
      "Epoch: 7, loss = 1.0671812929213047\n",
      "Epoch: 8, loss = 1.0639504864811897\n",
      "Epoch: 9, loss = 1.0604023225605488\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.135892279446125\n",
      "Epoch: 1, loss = 1.1092957481741905\n",
      "Epoch: 2, loss = 1.0957747399806976\n",
      "Epoch: 3, loss = 1.0865038484334946\n",
      "Epoch: 4, loss = 1.0797196067869663\n",
      "Epoch: 5, loss = 1.0743040591478348\n",
      "Epoch: 6, loss = 1.0694315172731876\n",
      "Epoch: 7, loss = 1.0648171044886112\n",
      "Epoch: 8, loss = 1.060295905917883\n",
      "Epoch: 9, loss = 1.055794347077608\n",
      "\n",
      "T = 1\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.0915072386463485\n",
      "Epoch: 1, loss = 1.0681824137767157\n",
      "Epoch: 2, loss = 1.0585540731747942\n",
      "Epoch: 3, loss = 1.0543144941329956\n",
      "Epoch: 4, loss = 1.0506060992678006\n",
      "Epoch: 5, loss = 1.0471731424331665\n",
      "Epoch: 6, loss = 1.0436847731471062\n",
      "Epoch: 7, loss = 1.0401298875610032\n",
      "Epoch: 8, loss = 1.0363835071523986\n",
      "Epoch: 9, loss = 1.0327810222903886\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.0846989924709005\n",
      "Epoch: 1, loss = 1.053803061445554\n",
      "Epoch: 2, loss = 1.0473139509558678\n",
      "Epoch: 3, loss = 1.0438328608870506\n",
      "Epoch: 4, loss = 1.0403292352954547\n",
      "Epoch: 5, loss = 1.0361708079775174\n",
      "Epoch: 6, loss = 1.0323446467518804\n",
      "Epoch: 7, loss = 1.0279496585329373\n",
      "Epoch: 8, loss = 1.0235202908515928\n",
      "Epoch: 9, loss = 1.0187805145978928\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.087557817498843\n",
      "Epoch: 1, loss = 1.0625652248660724\n",
      "Epoch: 2, loss = 1.0549343128999074\n",
      "Epoch: 3, loss = 1.0502363766233125\n",
      "Epoch: 4, loss = 1.0458924149473507\n",
      "Epoch: 5, loss = 1.0416198149323463\n",
      "Epoch: 6, loss = 1.0375443796316783\n",
      "Epoch: 7, loss = 1.033640724917253\n",
      "Epoch: 8, loss = 1.0296911969780922\n",
      "Epoch: 9, loss = 1.0256947552164395\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.08382015923659\n",
      "Epoch: 1, loss = 1.0567770997683206\n",
      "Epoch: 2, loss = 1.0486316035191217\n",
      "Epoch: 3, loss = 1.045577198266983\n",
      "Epoch: 4, loss = 1.042316106458505\n",
      "Epoch: 5, loss = 1.0385406414667766\n",
      "Epoch: 6, loss = 1.0347505807876587\n",
      "Epoch: 7, loss = 1.030883366862933\n",
      "Epoch: 8, loss = 1.026731602847576\n",
      "Epoch: 9, loss = 1.0223923499385519\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.0782628903786344\n",
      "Epoch: 1, loss = 1.051223600904147\n",
      "Epoch: 2, loss = 1.0436518117785454\n",
      "Epoch: 3, loss = 1.040040522813797\n",
      "Epoch: 4, loss = 1.036380834877491\n",
      "Epoch: 5, loss = 1.032674819231033\n",
      "Epoch: 6, loss = 1.0288508584101994\n",
      "Epoch: 7, loss = 1.0245957349737485\n",
      "Epoch: 8, loss = 1.0200867975751557\n",
      "Epoch: 9, loss = 1.015961786111196\n",
      "\n",
      "T = 2\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.054879929870367\n",
      "Epoch: 1, loss = 1.0411820001900196\n",
      "Epoch: 2, loss = 1.0365468971431255\n",
      "Epoch: 3, loss = 1.0334398560225964\n",
      "Epoch: 4, loss = 1.0287795942276716\n",
      "Epoch: 5, loss = 1.023760287091136\n",
      "Epoch: 6, loss = 1.01993677765131\n",
      "Epoch: 7, loss = 1.013111500069499\n",
      "Epoch: 8, loss = 1.0105158183723688\n",
      "Epoch: 9, loss = 1.0019407626241446\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.0468834470957518\n",
      "Epoch: 1, loss = 1.0275393147021532\n",
      "Epoch: 2, loss = 1.026137599721551\n",
      "Epoch: 3, loss = 1.020490912720561\n",
      "Epoch: 4, loss = 1.015589764341712\n",
      "Epoch: 5, loss = 1.009659020230174\n",
      "Epoch: 6, loss = 1.0036406870931387\n",
      "Epoch: 7, loss = 0.9980167746543884\n",
      "Epoch: 8, loss = 0.9916990958154202\n",
      "Epoch: 9, loss = 0.9854366183280945\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.0507761258631945\n",
      "Epoch: 1, loss = 1.0357392355799675\n",
      "Epoch: 2, loss = 1.0331581942737103\n",
      "Epoch: 3, loss = 1.0289206523448229\n",
      "Epoch: 4, loss = 1.0245716534554958\n",
      "Epoch: 5, loss = 1.0199440196156502\n",
      "Epoch: 6, loss = 1.0148765668272972\n",
      "Epoch: 7, loss = 1.0089190863072872\n",
      "Epoch: 8, loss = 1.0032710898667574\n",
      "Epoch: 9, loss = 0.9971188772469759\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.0507038533687592\n",
      "Epoch: 1, loss = 1.0343589298427105\n",
      "Epoch: 2, loss = 1.0314387418329716\n",
      "Epoch: 3, loss = 1.0275893341749907\n",
      "Epoch: 4, loss = 1.0227626394480467\n",
      "Epoch: 5, loss = 1.017117591574788\n",
      "Epoch: 6, loss = 1.012249505147338\n",
      "Epoch: 7, loss = 1.0070915147662163\n",
      "Epoch: 8, loss = 1.0008098054677248\n",
      "Epoch: 9, loss = 0.9956909138709307\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.0402630250900984\n",
      "Epoch: 1, loss = 1.0236630123108625\n",
      "Epoch: 2, loss = 1.0205262079834938\n",
      "Epoch: 3, loss = 1.0158987008035183\n",
      "Epoch: 4, loss = 1.0103441085666418\n",
      "Epoch: 5, loss = 1.0047852639108896\n",
      "Epoch: 6, loss = 0.9987908247858286\n",
      "Epoch: 7, loss = 0.9928861111402512\n",
      "Epoch: 8, loss = 0.9864190574735403\n",
      "Epoch: 9, loss = 0.9795319549739361\n",
      "\n",
      "T = 3\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.0290715500712395\n",
      "Epoch: 1, loss = 1.022744770348072\n",
      "Epoch: 2, loss = 1.0125003024935721\n",
      "Epoch: 3, loss = 1.0092226788401604\n",
      "Epoch: 4, loss = 1.0004099696874618\n",
      "Epoch: 5, loss = 0.9967635199427602\n",
      "Epoch: 6, loss = 0.9862992107868194\n",
      "Epoch: 7, loss = 0.9835320323705674\n",
      "Epoch: 8, loss = 0.9708745509386064\n",
      "Epoch: 9, loss = 0.9690490052103997\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.0090833172202112\n",
      "Epoch: 1, loss = 0.9974074855446817\n",
      "Epoch: 2, loss = 0.9880658999085428\n",
      "Epoch: 3, loss = 0.9799952849745752\n",
      "Epoch: 4, loss = 0.9720065385103224\n",
      "Epoch: 5, loss = 0.9628671407699585\n",
      "Epoch: 6, loss = 0.9536556318402292\n",
      "Epoch: 7, loss = 0.9441355124115942\n",
      "Epoch: 8, loss = 0.9337861567735672\n",
      "Epoch: 9, loss = 0.923587666451931\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.0231728553771973\n",
      "Epoch: 1, loss = 1.0136709034442901\n",
      "Epoch: 2, loss = 1.0047659948468206\n",
      "Epoch: 3, loss = 0.9987864300608637\n",
      "Epoch: 4, loss = 0.9912097871303556\n",
      "Epoch: 5, loss = 0.9834963262081146\n",
      "Epoch: 6, loss = 0.9756142303347587\n",
      "Epoch: 7, loss = 0.9667727217078209\n",
      "Epoch: 8, loss = 0.9583850979804993\n",
      "Epoch: 9, loss = 0.948988376557827\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.0217949926853178\n",
      "Epoch: 1, loss = 1.01360844373703\n",
      "Epoch: 2, loss = 1.0024867907166481\n",
      "Epoch: 3, loss = 0.9966777786612511\n",
      "Epoch: 4, loss = 0.9885371625423431\n",
      "Epoch: 5, loss = 0.9811932906508447\n",
      "Epoch: 6, loss = 0.97200795263052\n",
      "Epoch: 7, loss = 0.9640968635678294\n",
      "Epoch: 8, loss = 0.9544290110468865\n",
      "Epoch: 9, loss = 0.9457134798169133\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.0045183137059213\n",
      "Epoch: 1, loss = 0.9961258351802827\n",
      "Epoch: 2, loss = 0.985728611052036\n",
      "Epoch: 3, loss = 0.9778817653656009\n",
      "Epoch: 4, loss = 0.9698976546525954\n",
      "Epoch: 5, loss = 0.9611444875597951\n",
      "Epoch: 6, loss = 0.9526356279850007\n",
      "Epoch: 7, loss = 0.9430843338370323\n",
      "Epoch: 8, loss = 0.933659942448139\n",
      "Epoch: 9, loss = 0.92326235473156\n",
      "\n",
      "T = 4\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.9776378087699416\n",
      "Epoch: 1, loss = 0.9847218394279479\n",
      "Epoch: 2, loss = 0.961005916198095\n",
      "Epoch: 3, loss = 0.9574944737056892\n",
      "Epoch: 4, loss = 0.9424444821973643\n",
      "Epoch: 5, loss = 0.9368181737760705\n",
      "Epoch: 6, loss = 0.9222401703397431\n",
      "Epoch: 7, loss = 0.9152503013610838\n",
      "Epoch: 8, loss = 0.9014791411658128\n",
      "Epoch: 9, loss = 0.8911840021610259\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.9261478334665298\n",
      "Epoch: 1, loss = 0.9218569795290628\n",
      "Epoch: 2, loss = 0.8988132588565348\n",
      "Epoch: 3, loss = 0.8890591946740948\n",
      "Epoch: 4, loss = 0.8736486012736955\n",
      "Epoch: 5, loss = 0.8594746664166454\n",
      "Epoch: 6, loss = 0.8439333513379099\n",
      "Epoch: 7, loss = 0.8286206970612207\n",
      "Epoch: 8, loss = 0.812158594528834\n",
      "Epoch: 9, loss = 0.7963581184546152\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.9605432823300364\n",
      "Epoch: 1, loss = 0.9570534701148666\n",
      "Epoch: 2, loss = 0.9389129951596262\n",
      "Epoch: 3, loss = 0.9312365961571534\n",
      "Epoch: 4, loss = 0.9183047426243626\n",
      "Epoch: 5, loss = 0.9068412929773334\n",
      "Epoch: 6, loss = 0.8943555802106857\n",
      "Epoch: 7, loss = 0.8818463943898678\n",
      "Epoch: 8, loss = 0.8689428766568502\n",
      "Epoch: 9, loss = 0.8548854676385719\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.9562214948236943\n",
      "Epoch: 1, loss = 0.9557070856293046\n",
      "Epoch: 2, loss = 0.9331049832204977\n",
      "Epoch: 3, loss = 0.9271042905747888\n",
      "Epoch: 4, loss = 0.9112867961327233\n",
      "Epoch: 5, loss = 0.9018370956182482\n",
      "Epoch: 6, loss = 0.8868272018929322\n",
      "Epoch: 7, loss = 0.8744855212668577\n",
      "Epoch: 8, loss = 0.860462937504053\n",
      "Epoch: 9, loss = 0.8475287357966106\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.9297441256543003\n",
      "Epoch: 1, loss = 0.9299314233163994\n",
      "Epoch: 2, loss = 0.9079384170472624\n",
      "Epoch: 3, loss = 0.8998926493028798\n",
      "Epoch: 4, loss = 0.8850481212139127\n",
      "Epoch: 5, loss = 0.8731772465010482\n",
      "Epoch: 6, loss = 0.8595708943903446\n",
      "Epoch: 7, loss = 0.8451244831085207\n",
      "Epoch: 8, loss = 0.8306274811426799\n",
      "Epoch: 9, loss = 0.8163759037852285\n",
      "\n",
      "T = 5\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.8994708795632632\n",
      "Epoch: 1, loss = 0.901949228984969\n",
      "Epoch: 2, loss = 0.8843318969011306\n",
      "Epoch: 3, loss = 0.8742907707180295\n",
      "Epoch: 4, loss = 0.8618949236614363\n",
      "Epoch: 5, loss = 0.8496078306010791\n",
      "Epoch: 6, loss = 0.8372913556439537\n",
      "Epoch: 7, loss = 0.8253500621233666\n",
      "Epoch: 8, loss = 0.8125643283128737\n",
      "Epoch: 9, loss = 0.8008627593517307\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.801854260265827\n",
      "Epoch: 1, loss = 0.7873344165938241\n",
      "Epoch: 2, loss = 0.7753241839153426\n",
      "Epoch: 3, loss = 0.7617286614009312\n",
      "Epoch: 4, loss = 0.7481543554791381\n",
      "Epoch: 5, loss = 0.7347420757370335\n",
      "Epoch: 6, loss = 0.7217954397201538\n",
      "Epoch: 7, loss = 0.7097104106630598\n",
      "Epoch: 8, loss = 0.6976381912827491\n",
      "Epoch: 9, loss = 0.6857235649866718\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.861689920936312\n",
      "Epoch: 1, loss = 0.857652277818748\n",
      "Epoch: 2, loss = 0.8406126829130309\n",
      "Epoch: 3, loss = 0.8286097299839771\n",
      "Epoch: 4, loss = 0.8157135269471577\n",
      "Epoch: 5, loss = 0.8021930041057727\n",
      "Epoch: 6, loss = 0.7887792922556399\n",
      "Epoch: 7, loss = 0.7750683155442987\n",
      "Epoch: 8, loss = 0.7620237922029837\n",
      "Epoch: 9, loss = 0.7491971249026912\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.8519804956657546\n",
      "Epoch: 1, loss = 0.8506737521716528\n",
      "Epoch: 2, loss = 0.8306586997849601\n",
      "Epoch: 3, loss = 0.8194597406046729\n",
      "Epoch: 4, loss = 0.8062908692019328\n",
      "Epoch: 5, loss = 0.7931141427585056\n",
      "Epoch: 6, loss = 0.7790788843163422\n",
      "Epoch: 7, loss = 0.7667101626949651\n",
      "Epoch: 8, loss = 0.7534688924040112\n",
      "Epoch: 9, loss = 0.7412548852818354\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.8222761728933877\n",
      "Epoch: 1, loss = 0.8144918595041547\n",
      "Epoch: 2, loss = 0.7993660766099182\n",
      "Epoch: 3, loss = 0.7861975623028619\n",
      "Epoch: 4, loss = 0.7734176894383771\n",
      "Epoch: 5, loss = 0.7602029719523021\n",
      "Epoch: 6, loss = 0.7480419687926769\n",
      "Epoch: 7, loss = 0.736105857150895\n",
      "Epoch: 8, loss = 0.7245886549353601\n",
      "Epoch: 9, loss = 0.7138583527079648\n",
      "\n",
      "T = 6\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.7995515433140099\n",
      "Epoch: 1, loss = 0.7835198258981109\n",
      "Epoch: 2, loss = 0.7716377614997327\n",
      "Epoch: 3, loss = 0.7605119221843779\n",
      "Epoch: 4, loss = 0.7494904776103795\n",
      "Epoch: 5, loss = 0.7385319503955543\n",
      "Epoch: 6, loss = 0.7286089821718633\n",
      "Epoch: 7, loss = 0.7183020045049489\n",
      "Epoch: 8, loss = 0.709447312168777\n",
      "Epoch: 9, loss = 0.7002846752293408\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.6976967453956604\n",
      "Epoch: 1, loss = 0.6854176502674818\n",
      "Epoch: 2, loss = 0.6754856961779296\n",
      "Epoch: 3, loss = 0.6668725744821131\n",
      "Epoch: 4, loss = 0.6592666418291628\n",
      "Epoch: 5, loss = 0.6521325064823031\n",
      "Epoch: 6, loss = 0.6454268852248788\n",
      "Epoch: 7, loss = 0.6390867801383138\n",
      "Epoch: 8, loss = 0.6331994007341564\n",
      "Epoch: 9, loss = 0.6269909394904971\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.7552062752656639\n",
      "Epoch: 1, loss = 0.7368816453963518\n",
      "Epoch: 2, loss = 0.7257412285543978\n",
      "Epoch: 3, loss = 0.714706058613956\n",
      "Epoch: 4, loss = 0.7041369876824319\n",
      "Epoch: 5, loss = 0.6935074338689446\n",
      "Epoch: 6, loss = 0.6835546176880598\n",
      "Epoch: 7, loss = 0.6746029201894999\n",
      "Epoch: 8, loss = 0.6659084972925484\n",
      "Epoch: 9, loss = 0.6582515104673803\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.7473463332280517\n",
      "Epoch: 1, loss = 0.7304438706487417\n",
      "Epoch: 2, loss = 0.7187071996740997\n",
      "Epoch: 3, loss = 0.7081795944832265\n",
      "Epoch: 4, loss = 0.6975601124577224\n",
      "Epoch: 5, loss = 0.6870046840049326\n",
      "Epoch: 6, loss = 0.676853989250958\n",
      "Epoch: 7, loss = 0.6665606158785522\n",
      "Epoch: 8, loss = 0.6577058811672032\n",
      "Epoch: 9, loss = 0.6489098179154098\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.7277422300539911\n",
      "Epoch: 1, loss = 0.7150271642021835\n",
      "Epoch: 2, loss = 0.7055565281771123\n",
      "Epoch: 3, loss = 0.6978749237023294\n",
      "Epoch: 4, loss = 0.6904012802988291\n",
      "Epoch: 5, loss = 0.6834557228721678\n",
      "Epoch: 6, loss = 0.6762914313003421\n",
      "Epoch: 7, loss = 0.6690458706580102\n",
      "Epoch: 8, loss = 0.6628160127438605\n",
      "Epoch: 9, loss = 0.6562812374904752\n",
      "\n",
      "T = 7\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.7018807919489011\n",
      "Epoch: 1, loss = 0.6931856195959779\n",
      "Epoch: 2, loss = 0.68677820223901\n",
      "Epoch: 3, loss = 0.6787824386523831\n",
      "Epoch: 4, loss = 0.6718188495271737\n",
      "Epoch: 5, loss = 0.6653405858410729\n",
      "Epoch: 6, loss = 0.6585516664716932\n",
      "Epoch: 7, loss = 0.6534201312396263\n",
      "Epoch: 8, loss = 0.6480735821856393\n",
      "Epoch: 9, loss = 0.641888385017713\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.6466753395895164\n",
      "Epoch: 1, loss = 0.6351867363684706\n",
      "Epoch: 2, loss = 0.6324525566564666\n",
      "Epoch: 3, loss = 0.6269831069641643\n",
      "Epoch: 4, loss = 0.6230438355770377\n",
      "Epoch: 5, loss = 0.6181141440239218\n",
      "Epoch: 6, loss = 0.6136198254923025\n",
      "Epoch: 7, loss = 0.6091851455469927\n",
      "Epoch: 8, loss = 0.6042051903075641\n",
      "Epoch: 9, loss = 0.5996357219086752\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.668834629158179\n",
      "Epoch: 1, loss = 0.659261353313923\n",
      "Epoch: 2, loss = 0.6545647742847598\n",
      "Epoch: 3, loss = 0.6484758800102604\n",
      "Epoch: 4, loss = 0.6432641910182105\n",
      "Epoch: 5, loss = 0.6372446550263299\n",
      "Epoch: 6, loss = 0.6306717863513364\n",
      "Epoch: 7, loss = 0.6265757050779128\n",
      "Epoch: 8, loss = 0.6204786859452722\n",
      "Epoch: 9, loss = 0.6150855922864543\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.6615780703723431\n",
      "Epoch: 1, loss = 0.6510053931011096\n",
      "Epoch: 2, loss = 0.6457673977646564\n",
      "Epoch: 3, loss = 0.6379476259979936\n",
      "Epoch: 4, loss = 0.6306951286064253\n",
      "Epoch: 5, loss = 0.6219064854085445\n",
      "Epoch: 6, loss = 0.6145414958397547\n",
      "Epoch: 7, loss = 0.6059895497229363\n",
      "Epoch: 8, loss = 0.5996419390042621\n",
      "Epoch: 9, loss = 0.591334592964914\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.667683210844795\n",
      "Epoch: 1, loss = 0.6582882834805381\n",
      "Epoch: 2, loss = 0.6532677085035377\n",
      "Epoch: 3, loss = 0.6483943412701287\n",
      "Epoch: 4, loss = 0.6427611911462413\n",
      "Epoch: 5, loss = 0.6381183038983079\n",
      "Epoch: 6, loss = 0.6322798451615704\n",
      "Epoch: 7, loss = 0.6279178394211662\n",
      "Epoch: 8, loss = 0.6225186226268608\n",
      "Epoch: 9, loss = 0.6162507935530609\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.1963923871517181\n",
      "Epoch: 1, loss = 1.1781985014677048\n",
      "Epoch: 2, loss = 1.1640804931521416\n",
      "Epoch: 3, loss = 1.151992343366146\n",
      "Epoch: 4, loss = 1.1411870568990707\n",
      "Epoch: 5, loss = 1.1315441280603409\n",
      "Epoch: 6, loss = 1.1228797882795334\n",
      "Epoch: 7, loss = 1.1152742505073547\n",
      "Epoch: 8, loss = 1.1086474433541298\n",
      "Epoch: 9, loss = 1.1027466654777527\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.2020178884267807\n",
      "Epoch: 1, loss = 1.1823408156633377\n",
      "Epoch: 2, loss = 1.1670318990945816\n",
      "Epoch: 3, loss = 1.1535324826836586\n",
      "Epoch: 4, loss = 1.1409879475831985\n",
      "Epoch: 5, loss = 1.129554532468319\n",
      "Epoch: 6, loss = 1.1193057894706726\n",
      "Epoch: 7, loss = 1.1101637110114098\n",
      "Epoch: 8, loss = 1.1019703969359398\n",
      "Epoch: 9, loss = 1.0947442203760147\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.215959295630455\n",
      "Epoch: 1, loss = 1.1958709955215454\n",
      "Epoch: 2, loss = 1.182001680135727\n",
      "Epoch: 3, loss = 1.1694466471672058\n",
      "Epoch: 4, loss = 1.157359391450882\n",
      "Epoch: 5, loss = 1.1455898359417915\n",
      "Epoch: 6, loss = 1.1342896595597267\n",
      "Epoch: 7, loss = 1.1236381232738495\n",
      "Epoch: 8, loss = 1.1141643524169922\n",
      "Epoch: 9, loss = 1.105368360877037\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.2138453274965286\n",
      "Epoch: 1, loss = 1.193967878818512\n",
      "Epoch: 2, loss = 1.1777970492839813\n",
      "Epoch: 3, loss = 1.1627441048622131\n",
      "Epoch: 4, loss = 1.1492508798837662\n",
      "Epoch: 5, loss = 1.1372401714324951\n",
      "Epoch: 6, loss = 1.1261292845010757\n",
      "Epoch: 7, loss = 1.1158414781093597\n",
      "Epoch: 8, loss = 1.1066676154732704\n",
      "Epoch: 9, loss = 1.0986376404762268\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.21263986825943\n",
      "Epoch: 1, loss = 1.1916678696870804\n",
      "Epoch: 2, loss = 1.1749736815690994\n",
      "Epoch: 3, loss = 1.159878358244896\n",
      "Epoch: 4, loss = 1.1462118476629257\n",
      "Epoch: 5, loss = 1.1339227557182312\n",
      "Epoch: 6, loss = 1.1230667680501938\n",
      "Epoch: 7, loss = 1.1130428165197372\n",
      "Epoch: 8, loss = 1.1040225625038147\n",
      "Epoch: 9, loss = 1.0959854871034622\n",
      "\n",
      "T = 0\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.170232040186723\n",
      "Epoch: 1, loss = 1.1173860008517902\n",
      "Epoch: 2, loss = 1.0928798764944074\n",
      "Epoch: 3, loss = 1.0794804419080417\n",
      "Epoch: 4, loss = 1.0721747527519863\n",
      "Epoch: 5, loss = 1.066866343220075\n",
      "Epoch: 6, loss = 1.0618864645560584\n",
      "Epoch: 7, loss = 1.0569697096943853\n",
      "Epoch: 8, loss = 1.0518087421854336\n",
      "Epoch: 9, loss = 1.0476461822787921\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.174283710618814\n",
      "Epoch: 1, loss = 1.106611077984174\n",
      "Epoch: 2, loss = 1.0825996672113736\n",
      "Epoch: 3, loss = 1.0705880870421727\n",
      "Epoch: 4, loss = 1.0654796610275903\n",
      "Epoch: 5, loss = 1.0606644228100777\n",
      "Epoch: 6, loss = 1.0548945168654122\n",
      "Epoch: 7, loss = 1.0498879651228588\n",
      "Epoch: 8, loss = 1.043509085973104\n",
      "Epoch: 9, loss = 1.038235420982043\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.1705747619271278\n",
      "Epoch: 1, loss = 1.1155005122224488\n",
      "Epoch: 2, loss = 1.0916169186433156\n",
      "Epoch: 3, loss = 1.0760461489359536\n",
      "Epoch: 4, loss = 1.0683105687300365\n",
      "Epoch: 5, loss = 1.0628373002012574\n",
      "Epoch: 6, loss = 1.0570952246586482\n",
      "Epoch: 7, loss = 1.0518956755598383\n",
      "Epoch: 8, loss = 1.0465577517946558\n",
      "Epoch: 9, loss = 1.0412116845448813\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.157159057756265\n",
      "Epoch: 1, loss = 1.0993956997990608\n",
      "Epoch: 2, loss = 1.0766160661975541\n",
      "Epoch: 3, loss = 1.0667882983883221\n",
      "Epoch: 4, loss = 1.0623013153672216\n",
      "Epoch: 5, loss = 1.0579194699724517\n",
      "Epoch: 6, loss = 1.053261009355386\n",
      "Epoch: 7, loss = 1.0486914689342182\n",
      "Epoch: 8, loss = 1.044111100335916\n",
      "Epoch: 9, loss = 1.0392028292020163\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.1756630068024\n",
      "Epoch: 1, loss = 1.1140412017703059\n",
      "Epoch: 2, loss = 1.0876264745990436\n",
      "Epoch: 3, loss = 1.0721580535173414\n",
      "Epoch: 4, loss = 1.06398161003987\n",
      "Epoch: 5, loss = 1.059416430691878\n",
      "Epoch: 6, loss = 1.0549722562233605\n",
      "Epoch: 7, loss = 1.0500911176204681\n",
      "Epoch: 8, loss = 1.0451975564161937\n",
      "Epoch: 9, loss = 1.0401824836929638\n",
      "\n",
      "T = 1\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.0642201811075205\n",
      "Epoch: 1, loss = 1.0539459571242331\n",
      "Epoch: 2, loss = 1.0498010262846946\n",
      "Epoch: 3, loss = 1.0446138247847554\n",
      "Epoch: 4, loss = 1.0399195119738578\n",
      "Epoch: 5, loss = 1.0349657416343687\n",
      "Epoch: 6, loss = 1.0293018221855164\n",
      "Epoch: 7, loss = 1.0247579500079156\n",
      "Epoch: 8, loss = 1.0178807526826859\n",
      "Epoch: 9, loss = 1.012489928305149\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.0627358466386796\n",
      "Epoch: 1, loss = 1.0468616366386414\n",
      "Epoch: 2, loss = 1.0439498960971836\n",
      "Epoch: 3, loss = 1.0382358029484748\n",
      "Epoch: 4, loss = 1.0330412343144415\n",
      "Epoch: 5, loss = 1.0272213235497474\n",
      "Epoch: 6, loss = 1.0212472349405288\n",
      "Epoch: 7, loss = 1.0151696786284445\n",
      "Epoch: 8, loss = 1.008257919549942\n",
      "Epoch: 9, loss = 1.0015297397971152\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.0651311576366427\n",
      "Epoch: 1, loss = 1.0520830199122428\n",
      "Epoch: 2, loss = 1.0492153972387317\n",
      "Epoch: 3, loss = 1.0446373268961908\n",
      "Epoch: 4, loss = 1.0399047955870626\n",
      "Epoch: 5, loss = 1.0348133981227876\n",
      "Epoch: 6, loss = 1.029582932591438\n",
      "Epoch: 7, loss = 1.0240323990583418\n",
      "Epoch: 8, loss = 1.0175409957766535\n",
      "Epoch: 9, loss = 1.0113750219345092\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.0636363670229916\n",
      "Epoch: 1, loss = 1.0531623393297194\n",
      "Epoch: 2, loss = 1.049884536862373\n",
      "Epoch: 3, loss = 1.0454394176602362\n",
      "Epoch: 4, loss = 1.04073755890131\n",
      "Epoch: 5, loss = 1.036098776757717\n",
      "Epoch: 6, loss = 1.0309930339455604\n",
      "Epoch: 7, loss = 1.025063881278038\n",
      "Epoch: 8, loss = 1.0193376436829567\n",
      "Epoch: 9, loss = 1.0130406692624092\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.0619855135679246\n",
      "Epoch: 1, loss = 1.0497824132442473\n",
      "Epoch: 2, loss = 1.0456544190645216\n",
      "Epoch: 3, loss = 1.0404493585228918\n",
      "Epoch: 4, loss = 1.035150320827961\n",
      "Epoch: 5, loss = 1.029366783797741\n",
      "Epoch: 6, loss = 1.0235649421811104\n",
      "Epoch: 7, loss = 1.0171732082962992\n",
      "Epoch: 8, loss = 1.0103502944111824\n",
      "Epoch: 9, loss = 1.0038331210613252\n",
      "\n",
      "T = 2\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.0539384356566837\n",
      "Epoch: 1, loss = 1.0676630937627385\n",
      "Epoch: 2, loss = 1.0481725484132767\n",
      "Epoch: 3, loss = 1.0441797247954778\n",
      "Epoch: 4, loss = 1.0350987921868051\n",
      "Epoch: 5, loss = 1.028375258403165\n",
      "Epoch: 6, loss = 1.0191744821412225\n",
      "Epoch: 7, loss = 1.01247139275074\n",
      "Epoch: 8, loss = 1.0010445394686287\n",
      "Epoch: 9, loss = 0.9934710679309706\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.0396253573043004\n",
      "Epoch: 1, loss = 1.0431151560374672\n",
      "Epoch: 2, loss = 1.0246063321828844\n",
      "Epoch: 3, loss = 1.0153654930847034\n",
      "Epoch: 4, loss = 1.0050407360707012\n",
      "Epoch: 5, loss = 0.994608332003866\n",
      "Epoch: 6, loss = 0.9837350206715724\n",
      "Epoch: 7, loss = 0.9724927310432707\n",
      "Epoch: 8, loss = 0.9599725529551508\n",
      "Epoch: 9, loss = 0.9477037180747306\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.0530983667288512\n",
      "Epoch: 1, loss = 1.0594079760568482\n",
      "Epoch: 2, loss = 1.040555569742407\n",
      "Epoch: 3, loss = 1.033979719238622\n",
      "Epoch: 4, loss = 1.0233964760388645\n",
      "Epoch: 5, loss = 1.0141336896589823\n",
      "Epoch: 6, loss = 1.0027994534799032\n",
      "Epoch: 7, loss = 0.9926256111689975\n",
      "Epoch: 8, loss = 0.9804772159882953\n",
      "Epoch: 9, loss = 0.9687359790716852\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.0526124175105778\n",
      "Epoch: 1, loss = 1.0646028476102016\n",
      "Epoch: 2, loss = 1.0441086132611548\n",
      "Epoch: 3, loss = 1.037135683000088\n",
      "Epoch: 4, loss = 1.027398844914777\n",
      "Epoch: 5, loss = 1.01900104539735\n",
      "Epoch: 6, loss = 1.0084432267716952\n",
      "Epoch: 7, loss = 0.9981750760759627\n",
      "Epoch: 8, loss = 0.9874726533889768\n",
      "Epoch: 9, loss = 0.9767711141279767\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.0411537215113642\n",
      "Epoch: 1, loss = 1.0499149795089449\n",
      "Epoch: 2, loss = 1.0317457850490301\n",
      "Epoch: 3, loss = 1.0231659965855735\n",
      "Epoch: 4, loss = 1.012528383306095\n",
      "Epoch: 5, loss = 1.0021969346063477\n",
      "Epoch: 6, loss = 0.9910158078585353\n",
      "Epoch: 7, loss = 0.9794494350041659\n",
      "Epoch: 8, loss = 0.9676167379532542\n",
      "Epoch: 9, loss = 0.954653725028038\n",
      "\n",
      "T = 3\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.9834345645374724\n",
      "Epoch: 1, loss = 0.9822587809628912\n",
      "Epoch: 2, loss = 0.9572260065211188\n",
      "Epoch: 3, loss = 0.9479076282845602\n",
      "Epoch: 4, loss = 0.9264828744861812\n",
      "Epoch: 5, loss = 0.9123441701134045\n",
      "Epoch: 6, loss = 0.8926874755157364\n",
      "Epoch: 7, loss = 0.8751355989111794\n",
      "Epoch: 8, loss = 0.8567979128824339\n",
      "Epoch: 9, loss = 0.8393131519357361\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.932650685310364\n",
      "Epoch: 1, loss = 0.9158225879073145\n",
      "Epoch: 2, loss = 0.8955739537874856\n",
      "Epoch: 3, loss = 0.878447311619918\n",
      "Epoch: 4, loss = 0.8612181544303898\n",
      "Epoch: 5, loss = 0.8435430882705581\n",
      "Epoch: 6, loss = 0.8269735773404439\n",
      "Epoch: 7, loss = 0.8106005299422477\n",
      "Epoch: 8, loss = 0.7935967759953604\n",
      "Epoch: 9, loss = 0.778649576836162\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.9599093645811081\n",
      "Epoch: 1, loss = 0.9473943379190232\n",
      "Epoch: 2, loss = 0.9257615051335759\n",
      "Epoch: 3, loss = 0.9111891099148326\n",
      "Epoch: 4, loss = 0.8906596634123063\n",
      "Epoch: 5, loss = 0.8724690874417628\n",
      "Epoch: 6, loss = 0.8528351982434592\n",
      "Epoch: 7, loss = 0.8345278195208973\n",
      "Epoch: 8, loss = 0.81537331144015\n",
      "Epoch: 9, loss = 0.7953987543781597\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.9651563142736751\n",
      "Epoch: 1, loss = 0.9550808320442834\n",
      "Epoch: 2, loss = 0.9352229394846489\n",
      "Epoch: 3, loss = 0.9203888401389122\n",
      "Epoch: 4, loss = 0.9025404229760167\n",
      "Epoch: 5, loss = 0.8854593593213295\n",
      "Epoch: 6, loss = 0.8668788191345004\n",
      "Epoch: 7, loss = 0.8486979148454137\n",
      "Epoch: 8, loss = 0.829395196504063\n",
      "Epoch: 9, loss = 0.8112545915775827\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.9440142164627708\n",
      "Epoch: 1, loss = 0.9304908331897525\n",
      "Epoch: 2, loss = 0.9118216335773471\n",
      "Epoch: 3, loss = 0.8950779396626684\n",
      "Epoch: 4, loss = 0.8776604483524958\n",
      "Epoch: 5, loss = 0.8607260982195536\n",
      "Epoch: 6, loss = 0.8430014327168467\n",
      "Epoch: 7, loss = 0.8255223027533953\n",
      "Epoch: 8, loss = 0.8085212293598387\n",
      "Epoch: 9, loss = 0.7919609041677582\n",
      "\n",
      "T = 4\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.8241097039797086\n",
      "Epoch: 1, loss = 0.7952752350406214\n",
      "Epoch: 2, loss = 0.7771568135781722\n",
      "Epoch: 3, loss = 0.7598272338509563\n",
      "Epoch: 4, loss = 0.743413767354055\n",
      "Epoch: 5, loss = 0.7287214164706797\n",
      "Epoch: 6, loss = 0.7149224037473851\n",
      "Epoch: 7, loss = 0.7020966796712443\n",
      "Epoch: 8, loss = 0.690194504504854\n",
      "Epoch: 9, loss = 0.6796629537235606\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.7638247643004763\n",
      "Epoch: 1, loss = 0.7399114915593107\n",
      "Epoch: 2, loss = 0.7270991402593527\n",
      "Epoch: 3, loss = 0.714837002483281\n",
      "Epoch: 4, loss = 0.7042651721699671\n",
      "Epoch: 5, loss = 0.6936020722443409\n",
      "Epoch: 6, loss = 0.6834437108852645\n",
      "Epoch: 7, loss = 0.6749418459155342\n",
      "Epoch: 8, loss = 0.6664871062067422\n",
      "Epoch: 9, loss = 0.6576411713930692\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.7833993705836209\n",
      "Epoch: 1, loss = 0.7569358988919043\n",
      "Epoch: 2, loss = 0.7384304858066817\n",
      "Epoch: 3, loss = 0.7213410701264032\n",
      "Epoch: 4, loss = 0.7060011750595135\n",
      "Epoch: 5, loss = 0.6922357241538439\n",
      "Epoch: 6, loss = 0.6789456578818236\n",
      "Epoch: 7, loss = 0.6671810309317979\n",
      "Epoch: 8, loss = 0.6559110883284702\n",
      "Epoch: 9, loss = 0.6439789069647139\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.7979212870652024\n",
      "Epoch: 1, loss = 0.7685778916559436\n",
      "Epoch: 2, loss = 0.7498363463038744\n",
      "Epoch: 3, loss = 0.7323717170140961\n",
      "Epoch: 4, loss = 0.716626797887412\n",
      "Epoch: 5, loss = 0.7024676312099803\n",
      "Epoch: 6, loss = 0.6886983551084997\n",
      "Epoch: 7, loss = 0.6752109822224486\n",
      "Epoch: 8, loss = 0.6628642979670656\n",
      "Epoch: 9, loss = 0.6504059291698717\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.7852406599982217\n",
      "Epoch: 1, loss = 0.7620927498421887\n",
      "Epoch: 2, loss = 0.7475371868772943\n",
      "Epoch: 3, loss = 0.7350065091794186\n",
      "Epoch: 4, loss = 0.7234496379440482\n",
      "Epoch: 5, loss = 0.7138809185813773\n",
      "Epoch: 6, loss = 0.7041871706870472\n",
      "Epoch: 7, loss = 0.6947898573496127\n",
      "Epoch: 8, loss = 0.6863211176612162\n",
      "Epoch: 9, loss = 0.6776764264160935\n",
      "\n",
      "T = 5\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.6742412115518864\n",
      "Epoch: 1, loss = 0.6641142216439431\n",
      "Epoch: 2, loss = 0.6578428902878214\n",
      "Epoch: 3, loss = 0.6500210994138168\n",
      "Epoch: 4, loss = 0.643221703572915\n",
      "Epoch: 5, loss = 0.6358098126947878\n",
      "Epoch: 6, loss = 0.6288764941004605\n",
      "Epoch: 7, loss = 0.6222523746009053\n",
      "Epoch: 8, loss = 0.615288313191671\n",
      "Epoch: 9, loss = 0.608811418597515\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.6615547893139032\n",
      "Epoch: 1, loss = 0.6517121393520098\n",
      "Epoch: 2, loss = 0.6465327160862772\n",
      "Epoch: 3, loss = 0.6386013615589878\n",
      "Epoch: 4, loss = 0.6318112259300855\n",
      "Epoch: 5, loss = 0.6236102558099307\n",
      "Epoch: 6, loss = 0.6174220594649131\n",
      "Epoch: 7, loss = 0.6099454218951553\n",
      "Epoch: 8, loss = 0.6023023082659796\n",
      "Epoch: 9, loss = 0.5950922895796027\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.6432350744994786\n",
      "Epoch: 1, loss = 0.6293623249691269\n",
      "Epoch: 2, loss = 0.6215841411971128\n",
      "Epoch: 3, loss = 0.6124366527566543\n",
      "Epoch: 4, loss = 0.6023709375697835\n",
      "Epoch: 5, loss = 0.5930409554678661\n",
      "Epoch: 6, loss = 0.582402852148964\n",
      "Epoch: 7, loss = 0.5726554708985183\n",
      "Epoch: 8, loss = 0.5631213822903541\n",
      "Epoch: 9, loss = 0.5530555887291067\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.6443256131158425\n",
      "Epoch: 1, loss = 0.632364979157081\n",
      "Epoch: 2, loss = 0.6241099716952213\n",
      "Epoch: 3, loss = 0.6126397363841534\n",
      "Epoch: 4, loss = 0.6014606502766793\n",
      "Epoch: 5, loss = 0.5920381127641752\n",
      "Epoch: 6, loss = 0.5822086365750202\n",
      "Epoch: 7, loss = 0.572228643326805\n",
      "Epoch: 8, loss = 0.5629544464441445\n",
      "Epoch: 9, loss = 0.5535739488326583\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.6791743928423296\n",
      "Epoch: 1, loss = 0.6668173733812113\n",
      "Epoch: 2, loss = 0.6606495724274561\n",
      "Epoch: 3, loss = 0.6526666664733336\n",
      "Epoch: 4, loss = 0.6455986516980022\n",
      "Epoch: 5, loss = 0.6385997333205669\n",
      "Epoch: 6, loss = 0.6315199320132916\n",
      "Epoch: 7, loss = 0.6242103158281397\n",
      "Epoch: 8, loss = 0.6173007382223239\n",
      "Epoch: 9, loss = 0.6093199296066394\n",
      "\n",
      "T = 6\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.6277502869566283\n",
      "Epoch: 1, loss = 0.6275167596836887\n",
      "Epoch: 2, loss = 0.6261800480385623\n",
      "Epoch: 3, loss = 0.6186268799006942\n",
      "Epoch: 4, loss = 0.6120979843040306\n",
      "Epoch: 5, loss = 0.6053319831689199\n",
      "Epoch: 6, loss = 0.5991119196017584\n",
      "Epoch: 7, loss = 0.5938796635717151\n",
      "Epoch: 8, loss = 0.5877486181755862\n",
      "Epoch: 9, loss = 0.5824746136864025\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.6269589668760696\n",
      "Epoch: 1, loss = 0.6210976978143056\n",
      "Epoch: 2, loss = 0.6146901192764441\n",
      "Epoch: 3, loss = 0.6074817943076294\n",
      "Epoch: 4, loss = 0.6007412289579707\n",
      "Epoch: 5, loss = 0.5935407128185038\n",
      "Epoch: 6, loss = 0.5872924632082382\n",
      "Epoch: 7, loss = 0.580885566895207\n",
      "Epoch: 8, loss = 0.5741564108679691\n",
      "Epoch: 9, loss = 0.5675089620053767\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.5757178259392578\n",
      "Epoch: 1, loss = 0.5694610737264156\n",
      "Epoch: 2, loss = 0.5642455046375592\n",
      "Epoch: 3, loss = 0.5556673416246972\n",
      "Epoch: 4, loss = 0.5472180313120286\n",
      "Epoch: 5, loss = 0.5381719918300709\n",
      "Epoch: 6, loss = 0.5298330479611953\n",
      "Epoch: 7, loss = 0.5212788441528875\n",
      "Epoch: 8, loss = 0.5128081380079189\n",
      "Epoch: 9, loss = 0.5047702712317309\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.5826421573758126\n",
      "Epoch: 1, loss = 0.5780158246556917\n",
      "Epoch: 2, loss = 0.5746938212464253\n",
      "Epoch: 3, loss = 0.5645589078466094\n",
      "Epoch: 4, loss = 0.5566860843449831\n",
      "Epoch: 5, loss = 0.5478103837619226\n",
      "Epoch: 6, loss = 0.5394983839243652\n",
      "Epoch: 7, loss = 0.5307805494715772\n",
      "Epoch: 8, loss = 0.5222965062906347\n",
      "Epoch: 9, loss = 0.5142150385926169\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.6287028573453431\n",
      "Epoch: 1, loss = 0.6226017631590367\n",
      "Epoch: 2, loss = 0.6204325467348101\n",
      "Epoch: 3, loss = 0.6128415954609712\n",
      "Epoch: 4, loss = 0.6046631390849749\n",
      "Epoch: 5, loss = 0.5973472990095616\n",
      "Epoch: 6, loss = 0.5895326604445776\n",
      "Epoch: 7, loss = 0.5817165990670521\n",
      "Epoch: 8, loss = 0.5733828785518805\n",
      "Epoch: 9, loss = 0.5646463192999365\n",
      "\n",
      "T = 7\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.5928698331117629\n",
      "Epoch: 1, loss = 0.5888895892045075\n",
      "Epoch: 2, loss = 0.5815330855548383\n",
      "Epoch: 3, loss = 0.5760668349616669\n",
      "Epoch: 4, loss = 0.569672831388957\n",
      "Epoch: 5, loss = 0.5628065225832604\n",
      "Epoch: 6, loss = 0.556584224433583\n",
      "Epoch: 7, loss = 0.5503229171256809\n",
      "Epoch: 8, loss = 0.5445601122782511\n",
      "Epoch: 9, loss = 0.5383797765654675\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.573846140766845\n",
      "Epoch: 1, loss = 0.5625452795887697\n",
      "Epoch: 2, loss = 0.554872764274478\n",
      "Epoch: 3, loss = 0.5484317760257159\n",
      "Epoch: 4, loss = 0.541728950927363\n",
      "Epoch: 5, loss = 0.5355065596454285\n",
      "Epoch: 6, loss = 0.5290538768338807\n",
      "Epoch: 7, loss = 0.5221734647365179\n",
      "Epoch: 8, loss = 0.5157432149109594\n",
      "Epoch: 9, loss = 0.5090739133712998\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.5104693694190835\n",
      "Epoch: 1, loss = 0.5076614080325647\n",
      "Epoch: 2, loss = 0.49969541945713847\n",
      "Epoch: 3, loss = 0.493355946998824\n",
      "Epoch: 4, loss = 0.48651248049538803\n",
      "Epoch: 5, loss = 0.4795607054496512\n",
      "Epoch: 6, loss = 0.4732099965981701\n",
      "Epoch: 7, loss = 0.467286401113276\n",
      "Epoch: 8, loss = 0.46115784878459054\n",
      "Epoch: 9, loss = 0.45453474265249333\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.5219463338144122\n",
      "Epoch: 1, loss = 0.5163750300622162\n",
      "Epoch: 2, loss = 0.5076135734972709\n",
      "Epoch: 3, loss = 0.49943074578528884\n",
      "Epoch: 4, loss = 0.4917695291121217\n",
      "Epoch: 5, loss = 0.4843532814479924\n",
      "Epoch: 6, loss = 0.4771107021061813\n",
      "Epoch: 7, loss = 0.4696579409544082\n",
      "Epoch: 8, loss = 0.4624630811774883\n",
      "Epoch: 9, loss = 0.4553686648415509\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.5622228696284927\n",
      "Epoch: 1, loss = 0.5487574538325565\n",
      "Epoch: 2, loss = 0.5372205812703161\n",
      "Epoch: 3, loss = 0.5273686713155579\n",
      "Epoch: 4, loss = 0.5173581822172683\n",
      "Epoch: 5, loss = 0.5080225495073726\n",
      "Epoch: 6, loss = 0.497783486457432\n",
      "Epoch: 7, loss = 0.4886045000132396\n",
      "Epoch: 8, loss = 0.4790669547065215\n",
      "Epoch: 9, loss = 0.4695239201845492\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.1963923871517181\n",
      "Epoch: 1, loss = 1.1781985014677048\n",
      "Epoch: 2, loss = 1.1640804931521416\n",
      "Epoch: 3, loss = 1.151992343366146\n",
      "Epoch: 4, loss = 1.1411870568990707\n",
      "Epoch: 5, loss = 1.1315441280603409\n",
      "Epoch: 6, loss = 1.1228797882795334\n",
      "Epoch: 7, loss = 1.1152742505073547\n",
      "Epoch: 8, loss = 1.1086474433541298\n",
      "Epoch: 9, loss = 1.1027466654777527\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.2020178884267807\n",
      "Epoch: 1, loss = 1.1823408156633377\n",
      "Epoch: 2, loss = 1.1670318990945816\n",
      "Epoch: 3, loss = 1.1535324826836586\n",
      "Epoch: 4, loss = 1.1409879475831985\n",
      "Epoch: 5, loss = 1.129554532468319\n",
      "Epoch: 6, loss = 1.1193057894706726\n",
      "Epoch: 7, loss = 1.1101637110114098\n",
      "Epoch: 8, loss = 1.1019703969359398\n",
      "Epoch: 9, loss = 1.0947442203760147\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.215959295630455\n",
      "Epoch: 1, loss = 1.1958709955215454\n",
      "Epoch: 2, loss = 1.182001680135727\n",
      "Epoch: 3, loss = 1.1694466471672058\n",
      "Epoch: 4, loss = 1.157359391450882\n",
      "Epoch: 5, loss = 1.1455898359417915\n",
      "Epoch: 6, loss = 1.1342896595597267\n",
      "Epoch: 7, loss = 1.1236381232738495\n",
      "Epoch: 8, loss = 1.1141643524169922\n",
      "Epoch: 9, loss = 1.105368360877037\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.2138453274965286\n",
      "Epoch: 1, loss = 1.193967878818512\n",
      "Epoch: 2, loss = 1.1777970492839813\n",
      "Epoch: 3, loss = 1.1627441048622131\n",
      "Epoch: 4, loss = 1.1492508798837662\n",
      "Epoch: 5, loss = 1.1372401714324951\n",
      "Epoch: 6, loss = 1.1261292845010757\n",
      "Epoch: 7, loss = 1.1158414781093597\n",
      "Epoch: 8, loss = 1.1066676154732704\n",
      "Epoch: 9, loss = 1.0986376404762268\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.21263986825943\n",
      "Epoch: 1, loss = 1.1916678696870804\n",
      "Epoch: 2, loss = 1.1749736815690994\n",
      "Epoch: 3, loss = 1.159878358244896\n",
      "Epoch: 4, loss = 1.1462118476629257\n",
      "Epoch: 5, loss = 1.1339227557182312\n",
      "Epoch: 6, loss = 1.1230667680501938\n",
      "Epoch: 7, loss = 1.1130428165197372\n",
      "Epoch: 8, loss = 1.1040225625038147\n",
      "Epoch: 9, loss = 1.0959854871034622\n",
      "\n",
      "T = 0\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.1556801870465276\n",
      "Epoch: 1, loss = 1.0934023916721343\n",
      "Epoch: 2, loss = 1.066411152482033\n",
      "Epoch: 3, loss = 1.0665407955646518\n",
      "Epoch: 4, loss = 1.0617654964327814\n",
      "Epoch: 5, loss = 1.0554827347397804\n",
      "Epoch: 6, loss = 1.0504985779523848\n",
      "Epoch: 7, loss = 1.0443629920482635\n",
      "Epoch: 8, loss = 1.0386395975947382\n",
      "Epoch: 9, loss = 1.031714129447937\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.1495299637317657\n",
      "Epoch: 1, loss = 1.07928264439106\n",
      "Epoch: 2, loss = 1.0541070908308032\n",
      "Epoch: 3, loss = 1.0534696817398068\n",
      "Epoch: 4, loss = 1.0519460916519165\n",
      "Epoch: 5, loss = 1.0427530854940414\n",
      "Epoch: 6, loss = 1.0385627865791318\n",
      "Epoch: 7, loss = 1.033481813967228\n",
      "Epoch: 8, loss = 1.0280988425016402\n",
      "Epoch: 9, loss = 1.0229759126901625\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.1570710122585295\n",
      "Epoch: 1, loss = 1.099993617832661\n",
      "Epoch: 2, loss = 1.0656911134719846\n",
      "Epoch: 3, loss = 1.0595123231410981\n",
      "Epoch: 4, loss = 1.0563400015234952\n",
      "Epoch: 5, loss = 1.0505034729838372\n",
      "Epoch: 6, loss = 1.0442662596702574\n",
      "Epoch: 7, loss = 1.0392458334565164\n",
      "Epoch: 8, loss = 1.033321623504162\n",
      "Epoch: 9, loss = 1.0269101440906525\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.1390410199761392\n",
      "Epoch: 1, loss = 1.0732579112052918\n",
      "Epoch: 2, loss = 1.0566297113895418\n",
      "Epoch: 3, loss = 1.0563234165310862\n",
      "Epoch: 4, loss = 1.052427741885185\n",
      "Epoch: 5, loss = 1.0473327830433845\n",
      "Epoch: 6, loss = 1.041906988620758\n",
      "Epoch: 7, loss = 1.0370633497834207\n",
      "Epoch: 8, loss = 1.0322747975587845\n",
      "Epoch: 9, loss = 1.0269447043538091\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.1653890624642376\n",
      "Epoch: 1, loss = 1.0861331492662432\n",
      "Epoch: 2, loss = 1.0593011230230331\n",
      "Epoch: 3, loss = 1.0558932766318319\n",
      "Epoch: 4, loss = 1.05255835801363\n",
      "Epoch: 5, loss = 1.0476166144013406\n",
      "Epoch: 6, loss = 1.0415575727820396\n",
      "Epoch: 7, loss = 1.0358779206871986\n",
      "Epoch: 8, loss = 1.0301540061831476\n",
      "Epoch: 9, loss = 1.0241156280040742\n",
      "\n",
      "T = 1\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.1041551505525908\n",
      "Epoch: 1, loss = 1.1091933954093192\n",
      "Epoch: 2, loss = 1.0937257707118986\n",
      "Epoch: 3, loss = 1.0882167170445123\n",
      "Epoch: 4, loss = 1.0805703219440248\n",
      "Epoch: 5, loss = 1.0736261473761664\n",
      "Epoch: 6, loss = 1.0660991975002818\n",
      "Epoch: 7, loss = 1.0592552762892513\n",
      "Epoch: 8, loss = 1.0513953674170708\n",
      "Epoch: 9, loss = 1.0444565771354573\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.1055315766069627\n",
      "Epoch: 1, loss = 1.0960266002350385\n",
      "Epoch: 2, loss = 1.0828710546096163\n",
      "Epoch: 3, loss = 1.0763718146416876\n",
      "Epoch: 4, loss = 1.0678017139434812\n",
      "Epoch: 5, loss = 1.060410100552771\n",
      "Epoch: 6, loss = 1.0529907751414513\n",
      "Epoch: 7, loss = 1.043298284212748\n",
      "Epoch: 8, loss = 1.0344440398944748\n",
      "Epoch: 9, loss = 1.0246530928545532\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.1110812980267741\n",
      "Epoch: 1, loss = 1.105282494591342\n",
      "Epoch: 2, loss = 1.0909204847282832\n",
      "Epoch: 3, loss = 1.084486602909035\n",
      "Epoch: 4, loss = 1.0757602983050878\n",
      "Epoch: 5, loss = 1.0686202239659095\n",
      "Epoch: 6, loss = 1.0579475015401842\n",
      "Epoch: 7, loss = 1.0494791343808176\n",
      "Epoch: 8, loss = 1.038199704554346\n",
      "Epoch: 9, loss = 1.0273888218733997\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.1069961157110006\n",
      "Epoch: 1, loss = 1.111574348476198\n",
      "Epoch: 2, loss = 1.0946004059579635\n",
      "Epoch: 3, loss = 1.0883122972316213\n",
      "Epoch: 4, loss = 1.0816847019725377\n",
      "Epoch: 5, loss = 1.0743922607766259\n",
      "Epoch: 6, loss = 1.0681073723567855\n",
      "Epoch: 7, loss = 1.060689070986377\n",
      "Epoch: 8, loss = 1.0540650544895065\n",
      "Epoch: 9, loss = 1.0453536063432693\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.1021577301952574\n",
      "Epoch: 1, loss = 1.103252587219079\n",
      "Epoch: 2, loss = 1.089700551496612\n",
      "Epoch: 3, loss = 1.0817840753330124\n",
      "Epoch: 4, loss = 1.0745705382691493\n",
      "Epoch: 5, loss = 1.0654993040694132\n",
      "Epoch: 6, loss = 1.05799200054672\n",
      "Epoch: 7, loss = 1.048742620481385\n",
      "Epoch: 8, loss = 1.0395824089646335\n",
      "Epoch: 9, loss = 1.0300553060240216\n",
      "\n",
      "T = 2\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1.0594367694396238\n",
      "Epoch: 1, loss = 1.0267180238778777\n",
      "Epoch: 2, loss = 1.011267876395812\n",
      "Epoch: 3, loss = 0.9941474004433707\n",
      "Epoch: 4, loss = 0.9743755322236278\n",
      "Epoch: 5, loss = 0.9534704771179415\n",
      "Epoch: 6, loss = 0.9307637312091315\n",
      "Epoch: 7, loss = 0.9075212449981617\n",
      "Epoch: 8, loss = 0.8839177234241599\n",
      "Epoch: 9, loss = 0.8615966370472538\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1.0364209356216287\n",
      "Epoch: 1, loss = 0.9981472452099509\n",
      "Epoch: 2, loss = 0.9774458442743008\n",
      "Epoch: 3, loss = 0.9579691325242704\n",
      "Epoch: 4, loss = 0.9368410758101019\n",
      "Epoch: 5, loss = 0.9139668190708525\n",
      "Epoch: 6, loss = 0.889502219282664\n",
      "Epoch: 7, loss = 0.8633945329258073\n",
      "Epoch: 8, loss = 0.838676098161019\n",
      "Epoch: 9, loss = 0.812564282176586\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1.0392051001007738\n",
      "Epoch: 1, loss = 0.9997074495141326\n",
      "Epoch: 2, loss = 0.9786662488029556\n",
      "Epoch: 3, loss = 0.9569146329393751\n",
      "Epoch: 4, loss = 0.9328350098087238\n",
      "Epoch: 5, loss = 0.9084628473680755\n",
      "Epoch: 6, loss = 0.8840555861019175\n",
      "Epoch: 7, loss = 0.8600717103825164\n",
      "Epoch: 8, loss = 0.8349761249354254\n",
      "Epoch: 9, loss = 0.8112105638361894\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1.0650715409563136\n",
      "Epoch: 1, loss = 1.0328090483179457\n",
      "Epoch: 2, loss = 1.0133336513088302\n",
      "Epoch: 3, loss = 0.9960292640786906\n",
      "Epoch: 4, loss = 0.9769075214862822\n",
      "Epoch: 5, loss = 0.9556200401141095\n",
      "Epoch: 6, loss = 0.9322705028148797\n",
      "Epoch: 7, loss = 0.9073683023452755\n",
      "Epoch: 8, loss = 0.8820925429463388\n",
      "Epoch: 9, loss = 0.8580865114927293\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1.0483811239783578\n",
      "Epoch: 1, loss = 1.0134716251721752\n",
      "Epoch: 2, loss = 0.9945006639911578\n",
      "Epoch: 3, loss = 0.9755925387144088\n",
      "Epoch: 4, loss = 0.9539922538858192\n",
      "Epoch: 5, loss = 0.9318142616404937\n",
      "Epoch: 6, loss = 0.9084955579959428\n",
      "Epoch: 7, loss = 0.8850128160646326\n",
      "Epoch: 8, loss = 0.8601514376126804\n",
      "Epoch: 9, loss = 0.837014064479333\n",
      "\n",
      "T = 3\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.8155380393214084\n",
      "Epoch: 1, loss = 0.7974729564260032\n",
      "Epoch: 2, loss = 0.7764001144644095\n",
      "Epoch: 3, loss = 0.7558735699995478\n",
      "Epoch: 4, loss = 0.7365496039171426\n",
      "Epoch: 5, loss = 0.7187590994598236\n",
      "Epoch: 6, loss = 0.701740737456609\n",
      "Epoch: 7, loss = 0.6860559917986394\n",
      "Epoch: 8, loss = 0.6706498970880229\n",
      "Epoch: 9, loss = 0.6563588708420011\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.7777211648576401\n",
      "Epoch: 1, loss = 0.7583302456666443\n",
      "Epoch: 2, loss = 0.7383210994303225\n",
      "Epoch: 3, loss = 0.7193829901516438\n",
      "Epoch: 4, loss = 0.7027438780402432\n",
      "Epoch: 5, loss = 0.6867725980632445\n",
      "Epoch: 6, loss = 0.6718721906928454\n",
      "Epoch: 7, loss = 0.6582334740635227\n",
      "Epoch: 8, loss = 0.6457230293137184\n",
      "Epoch: 9, loss = 0.6331320252707776\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.7752981437918014\n",
      "Epoch: 1, loss = 0.7585707865655419\n",
      "Epoch: 2, loss = 0.7413119572926973\n",
      "Epoch: 3, loss = 0.7242499322575683\n",
      "Epoch: 4, loss = 0.7096590445760418\n",
      "Epoch: 5, loss = 0.6948738470673561\n",
      "Epoch: 6, loss = 0.6809523770695222\n",
      "Epoch: 7, loss = 0.6673673683887015\n",
      "Epoch: 8, loss = 0.6529196678277321\n",
      "Epoch: 9, loss = 0.6400474162005328\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.8152279763975566\n",
      "Epoch: 1, loss = 0.7889675890259883\n",
      "Epoch: 2, loss = 0.7704257797449828\n",
      "Epoch: 3, loss = 0.7503982891931257\n",
      "Epoch: 4, loss = 0.7322942294618663\n",
      "Epoch: 5, loss = 0.7147757723270096\n",
      "Epoch: 6, loss = 0.6954937900471339\n",
      "Epoch: 7, loss = 0.6778319182203095\n",
      "Epoch: 8, loss = 0.660500818534809\n",
      "Epoch: 9, loss = 0.6449795710470745\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.7930931889397257\n",
      "Epoch: 1, loss = 0.7713416586903965\n",
      "Epoch: 2, loss = 0.7537580918082416\n",
      "Epoch: 3, loss = 0.735437613309306\n",
      "Epoch: 4, loss = 0.7190324520582663\n",
      "Epoch: 5, loss = 0.7027439579148502\n",
      "Epoch: 6, loss = 0.6870926707325613\n",
      "Epoch: 7, loss = 0.6733594738385256\n",
      "Epoch: 8, loss = 0.659975480288267\n",
      "Epoch: 9, loss = 0.6461793401223773\n",
      "\n",
      "T = 4\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.6025588782060713\n",
      "Epoch: 1, loss = 0.600754834356762\n",
      "Epoch: 2, loss = 0.5846751476859763\n",
      "Epoch: 3, loss = 0.5715855339630727\n",
      "Epoch: 4, loss = 0.5581620381701561\n",
      "Epoch: 5, loss = 0.546245505767209\n",
      "Epoch: 6, loss = 0.5343402703957898\n",
      "Epoch: 7, loss = 0.5240349546074866\n",
      "Epoch: 8, loss = 0.5131094915171465\n",
      "Epoch: 9, loss = 0.5037393786589658\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.5867583250538225\n",
      "Epoch: 1, loss = 0.5846622729940072\n",
      "Epoch: 2, loss = 0.5714696582761548\n",
      "Epoch: 3, loss = 0.5582891741678829\n",
      "Epoch: 4, loss = 0.5451662080983316\n",
      "Epoch: 5, loss = 0.5314186843378202\n",
      "Epoch: 6, loss = 0.5175231802499011\n",
      "Epoch: 7, loss = 0.5036297370457933\n",
      "Epoch: 8, loss = 0.48941054155251806\n",
      "Epoch: 9, loss = 0.47353581787042703\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.5932957192084621\n",
      "Epoch: 1, loss = 0.5908640328617322\n",
      "Epoch: 2, loss = 0.5744891429231281\n",
      "Epoch: 3, loss = 0.5606297837304218\n",
      "Epoch: 4, loss = 0.5462936955903257\n",
      "Epoch: 5, loss = 0.5321188500771918\n",
      "Epoch: 6, loss = 0.5182250965209232\n",
      "Epoch: 7, loss = 0.5048469831013962\n",
      "Epoch: 8, loss = 0.49150929795134635\n",
      "Epoch: 9, loss = 0.4783996796412837\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.5928693476709581\n",
      "Epoch: 1, loss = 0.5921942638676793\n",
      "Epoch: 2, loss = 0.5763434833359155\n",
      "Epoch: 3, loss = 0.5629148955146471\n",
      "Epoch: 4, loss = 0.5493440193434558\n",
      "Epoch: 5, loss = 0.5364530796983413\n",
      "Epoch: 6, loss = 0.5233146013425931\n",
      "Epoch: 7, loss = 0.5105101178799356\n",
      "Epoch: 8, loss = 0.49827683233611636\n",
      "Epoch: 9, loss = 0.4854331677779554\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.5926666206547191\n",
      "Epoch: 1, loss = 0.5883210875271333\n",
      "Epoch: 2, loss = 0.5709000913692368\n",
      "Epoch: 3, loss = 0.5547169025632597\n",
      "Epoch: 4, loss = 0.5380801683557881\n",
      "Epoch: 5, loss = 0.5220088523236054\n",
      "Epoch: 6, loss = 0.506054816811922\n",
      "Epoch: 7, loss = 0.48974367350872083\n",
      "Epoch: 8, loss = 0.4740470103653409\n",
      "Epoch: 9, loss = 0.45746662394542786\n",
      "\n",
      "T = 5\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.5049581802543252\n",
      "Epoch: 1, loss = 0.5105056457221507\n",
      "Epoch: 2, loss = 0.49568230245728057\n",
      "Epoch: 3, loss = 0.48428038556128744\n",
      "Epoch: 4, loss = 0.4747694651037456\n",
      "Epoch: 5, loss = 0.46514610016718516\n",
      "Epoch: 6, loss = 0.4562258221954107\n",
      "Epoch: 7, loss = 0.44773452369496225\n",
      "Epoch: 8, loss = 0.43903405332937817\n",
      "Epoch: 9, loss = 0.42992808685638\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.4579959514364604\n",
      "Epoch: 1, loss = 0.45911930281668895\n",
      "Epoch: 2, loss = 0.4334830541070551\n",
      "Epoch: 3, loss = 0.4191072326386348\n",
      "Epoch: 4, loss = 0.4037303343135866\n",
      "Epoch: 5, loss = 0.38940267290920016\n",
      "Epoch: 6, loss = 0.3766043553780765\n",
      "Epoch: 7, loss = 0.3624605096154846\n",
      "Epoch: 8, loss = 0.3505007941462097\n",
      "Epoch: 9, loss = 0.3366595311742277\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.4620240057259798\n",
      "Epoch: 1, loss = 0.457817888967693\n",
      "Epoch: 2, loss = 0.44464266110211614\n",
      "Epoch: 3, loss = 0.4331656131893397\n",
      "Epoch: 4, loss = 0.42176151823252434\n",
      "Epoch: 5, loss = 0.4119838149845597\n",
      "Epoch: 6, loss = 0.40129477812908604\n",
      "Epoch: 7, loss = 0.3909108772315085\n",
      "Epoch: 8, loss = 0.380751665700227\n",
      "Epoch: 9, loss = 0.3710782184125857\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.47303839176893225\n",
      "Epoch: 1, loss = 0.47726777210831633\n",
      "Epoch: 2, loss = 0.4663568176701665\n",
      "Epoch: 3, loss = 0.45423428419977435\n",
      "Epoch: 4, loss = 0.4440011371113363\n",
      "Epoch: 5, loss = 0.43351845016703017\n",
      "Epoch: 6, loss = 0.42319253807887475\n",
      "Epoch: 7, loss = 0.4127454784139991\n",
      "Epoch: 8, loss = 0.4005223611509428\n",
      "Epoch: 9, loss = 0.3872742965398355\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.43737108347006126\n",
      "Epoch: 1, loss = 0.4384231946803629\n",
      "Epoch: 2, loss = 0.41433686401229364\n",
      "Epoch: 3, loss = 0.395844098776579\n",
      "Epoch: 4, loss = 0.37892505070893084\n",
      "Epoch: 5, loss = 0.3628171949321408\n",
      "Epoch: 6, loss = 0.3461111207073553\n",
      "Epoch: 7, loss = 0.3330242862412706\n",
      "Epoch: 8, loss = 0.31934668602421884\n",
      "Epoch: 9, loss = 0.30539814386516834\n",
      "\n",
      "T = 6\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.5072944888740716\n",
      "Epoch: 1, loss = 0.5305898767004816\n",
      "Epoch: 2, loss = 0.5221228106790383\n",
      "Epoch: 3, loss = 0.5140754471629344\n",
      "Epoch: 4, loss = 0.5056986935308266\n",
      "Epoch: 5, loss = 0.4987115741482585\n",
      "Epoch: 6, loss = 0.48932487379904055\n",
      "Epoch: 7, loss = 0.4825185983409656\n",
      "Epoch: 8, loss = 0.4736945083975025\n",
      "Epoch: 9, loss = 0.46937347979059985\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.4079743656369953\n",
      "Epoch: 1, loss = 0.4244437654149429\n",
      "Epoch: 2, loss = 0.4161480918470449\n",
      "Epoch: 3, loss = 0.40740779128150445\n",
      "Epoch: 4, loss = 0.39763684853798975\n",
      "Epoch: 5, loss = 0.38854692207151553\n",
      "Epoch: 6, loss = 0.3781286965300936\n",
      "Epoch: 7, loss = 0.36784272113461686\n",
      "Epoch: 8, loss = 0.35705212948057\n",
      "Epoch: 9, loss = 0.34476174893467854\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.43257172920355763\n",
      "Epoch: 1, loss = 0.46795712154487107\n",
      "Epoch: 2, loss = 0.45878245196393547\n",
      "Epoch: 3, loss = 0.44786446771166966\n",
      "Epoch: 4, loss = 0.4384141753283171\n",
      "Epoch: 5, loss = 0.4300787764328434\n",
      "Epoch: 6, loss = 0.4206311938910872\n",
      "Epoch: 7, loss = 0.4114989598767417\n",
      "Epoch: 8, loss = 0.4028249008556958\n",
      "Epoch: 9, loss = 0.3946662903000632\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.4704683602201461\n",
      "Epoch: 1, loss = 0.488092400109524\n",
      "Epoch: 2, loss = 0.4807562400699303\n",
      "Epoch: 3, loss = 0.4706574915767776\n",
      "Epoch: 4, loss = 0.4611176399088561\n",
      "Epoch: 5, loss = 0.452755728106271\n",
      "Epoch: 6, loss = 0.4441765165036739\n",
      "Epoch: 7, loss = 0.4360214167812457\n",
      "Epoch: 8, loss = 0.4285095834478349\n",
      "Epoch: 9, loss = 0.42091052944306295\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.3831414521113839\n",
      "Epoch: 1, loss = 0.40669728808329764\n",
      "Epoch: 2, loss = 0.3958017436932001\n",
      "Epoch: 3, loss = 0.3858042154160069\n",
      "Epoch: 4, loss = 0.37353499033809495\n",
      "Epoch: 5, loss = 0.360758131763352\n",
      "Epoch: 6, loss = 0.3490417241941783\n",
      "Epoch: 7, loss = 0.3380131636553541\n",
      "Epoch: 8, loss = 0.32681769006163014\n",
      "Epoch: 9, loss = 0.3162959388794444\n",
      "\n",
      "T = 7\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.5096568856370691\n",
      "Epoch: 1, loss = 0.4858343691607429\n",
      "Epoch: 2, loss = 0.47789590631146\n",
      "Epoch: 3, loss = 0.4727180681727599\n",
      "Epoch: 4, loss = 0.46594910723665195\n",
      "Epoch: 5, loss = 0.45942472813961394\n",
      "Epoch: 6, loss = 0.45288844127208\n",
      "Epoch: 7, loss = 0.44541001859629004\n",
      "Epoch: 8, loss = 0.4382246841648309\n",
      "Epoch: 9, loss = 0.4294078868326984\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.3759394200822115\n",
      "Epoch: 1, loss = 0.3555622761786887\n",
      "Epoch: 2, loss = 0.3408939733318137\n",
      "Epoch: 3, loss = 0.3287220236203265\n",
      "Epoch: 4, loss = 0.31610808920849\n",
      "Epoch: 5, loss = 0.3059937995540994\n",
      "Epoch: 6, loss = 0.29406731718424445\n",
      "Epoch: 7, loss = 0.2815450227812327\n",
      "Epoch: 8, loss = 0.27200544054821063\n",
      "Epoch: 9, loss = 0.26125326481379674\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.43146898023516034\n",
      "Epoch: 1, loss = 0.411295618202227\n",
      "Epoch: 2, loss = 0.3979813487085542\n",
      "Epoch: 3, loss = 0.3850425253304739\n",
      "Epoch: 4, loss = 0.37452181416323793\n",
      "Epoch: 5, loss = 0.365600519386741\n",
      "Epoch: 6, loss = 0.3561013328355991\n",
      "Epoch: 7, loss = 0.3473025699724642\n",
      "Epoch: 8, loss = 0.3395529184049475\n",
      "Epoch: 9, loss = 0.33106446592489996\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.464087880431731\n",
      "Epoch: 1, loss = 0.44028528757873137\n",
      "Epoch: 2, loss = 0.43132568733540866\n",
      "Epoch: 3, loss = 0.4227145010019849\n",
      "Epoch: 4, loss = 0.4144526133384331\n",
      "Epoch: 5, loss = 0.40758250218773795\n",
      "Epoch: 6, loss = 0.3997630609665067\n",
      "Epoch: 7, loss = 0.3924404625096498\n",
      "Epoch: 8, loss = 0.385209584315872\n",
      "Epoch: 9, loss = 0.3769994970114731\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.3484868763127563\n",
      "Epoch: 1, loss = 0.3308548445629917\n",
      "Epoch: 2, loss = 0.3186797670610815\n",
      "Epoch: 3, loss = 0.3082532443681195\n",
      "Epoch: 4, loss = 0.29791749880303436\n",
      "Epoch: 5, loss = 0.2876303313391734\n",
      "Epoch: 6, loss = 0.2776193986011838\n",
      "Epoch: 7, loss = 0.2687279837290672\n",
      "Epoch: 8, loss = 0.2585605679072834\n",
      "Epoch: 9, loss = 0.24829063028499307\n"
     ]
    }
   ],
   "source": [
    "# AL, ensemble of 5 NNs\n",
    "frac_err_AL_ens = []\n",
    "\n",
    "for i in range(len(K_train_list)):\n",
    "    \n",
    "    # Update dictiorary to have no active learning and the correct amount of points\n",
    "    config_AL[\"model_kwargs\"][\"config\"][\"num_models\"] = 5\n",
    "    config_AL[\"num_init\"] = ninit\n",
    "    config_AL[\"K\"] = K_train_list[i]\n",
    "    config_AL[\"M\"] = 4\n",
    "    config_AL[\"T\"] = T\n",
    "\n",
    "    # Instantiate the class object and train the model\n",
    "    uq_model = ActivelyLearnedModel(config=config_AL, device=device, online=False)\n",
    "    uq_model = uq_model.fit()\n",
    "\n",
    "    # Create a test dataset\n",
    "    X_test = sample_(int(config_AL[\"num_init\"] + config_AL[\"T\"]*config_AL[\"M\"]*config_AL[\"K\"]), int(N_test))\n",
    "    y_test = querry_(int(config_AL[\"num_init\"] + config_AL[\"T\"]*config_AL[\"M\"]*config_AL[\"K\"]), int(N_test))\n",
    "    y_test = np.reshape(y_test, (-1,))\n",
    "\n",
    "    res = uq_model.predict(X_test)\n",
    "    y_test_pred = np.squeeze(res.y_mean, axis=1)\n",
    "\n",
    "    frac_err_AL_ens.append(np.sqrt(np.sum(np.square(y_test - y_test_pred)))/np.sqrt(np.sum(np.square(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_train_list = ninit + T * np.array(K_train_list)\n",
    "plt.style.use(\"classic\")\n",
    "fig = plt.figure()\n",
    "plt.plot(N_train_list,frac_err_baseline, \".-\", label=\"Baseline\")\n",
    "plt.plot(N_train_list,frac_err_AL_ens, \".-\", label=\"Active learning, ensemble\")\n",
    "plt.ylabel('Fractional error on test set')\n",
    "plt.xlabel('Number of training points')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this figure that active learning enabled by an uncertainty quantification (such as the ones available with uq360) outperforms sampling at random significantly. For small numbers of training points, the actively learned surrogate has an error significantly smaller than the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
